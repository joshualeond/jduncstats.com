[
  {
    "path": "bayes-hackr/2021-01-08-bayes-hackr-ch1/",
    "title": "Chapter 1",
    "description": "Reproducing Chapter 1 of Bayesian Methods for Hackers in R + Stan",
    "author": [
      {
        "name": "J. Duncan",
        "url": {}
      }
    ],
    "date": "2021-01-08",
    "categories": [],
    "contents": "\n1.4 Using Computers to Perform Bayesian Inference for Us\n1.4.1 Example: Inferring Behavior from Text-Message Data\n\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(latex2exp)\n\nurl <- \"https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/data/txtdata.csv\"\n\ndata <- read_csv(url, col_names = FALSE) %>% \n  rename(count = X1) %>% \n  mutate(day = row_number())\n\n\n\nLet’s visualize this data to make sure all is well:\n\n\ntheme_set(theme_tidybayes())\n\ndata %>% \n  ggplot(aes(x=day, y=count)) + \n  geom_col() +\n  labs(\n    title = \"Did the user's texting habits change over time?\",\n    x = \"Time (days)\",\n    y = \"count of text-msgs received\"\n  ) \n\n\n\n\n1.4.2 Introducing Our First Hammer: PyMC Stan\nThis is where things get interesting. The original model included a discrete random variable \\(\\tau\\) that was modeled with a Discrete Uniform distribution. Then they sampled with the Metropolis algorithm so it was possible to sample from a joint distribution including this discrete random variable.\nA limitation (I suppose) of the main algorithm used by Stan, HMC, is that everything must be differentiable and so discrete random variables are off the table. However, with a little statistical theory you’re still able to estimate this model and even this exact posterior of \\(\\tau\\) by marginalizing the parameter out of the probability function. Then recovering this parameter after the fact in the generated quantities block!\nSee the Stan manual as well as this blog post for more info on implementing this model and the marginalization process.\n\n\ndata_list <- tidybayes::compose_data(data)\ndata_list[['alpha']] <- 1 / mean(data_list$count)\ndata_list\n\n\n$count\n [1] 13 24  8 24  7 35 14 11 15 11 22 22 11 57 11 19 29  6 19 12 22 12\n[23] 18 72 32  9  7 13 19 23 27 20  6 17 13 10 14  6 16 15  7  2 15 15\n[45] 19 70 49  7 53 22 21 31 19 11 18 20 12 35 17 23 17  4  2 31 30 13\n[67] 27  0 39 37  5 14 13 22\n\n$day\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n[23] 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n[45] 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66\n[67] 67 68 69 70 71 72 73 74\n\n$n\n[1] 74\n\n$alpha\n[1] 0.05065024\n\nHere’s the Stan code used for estimating this joint probability distribution:\n\ndata {\n  int<lower=1> n;\n  int<lower=0> count[n];\n  real alpha;\n}\n\ntransformed data {\n  real log_unif;\n  log_unif = -log(n);\n}\n\nparameters {\n  real<lower=0> lambda_1;\n  real<lower=0> lambda_2;\n}\n\ntransformed parameters {\n  vector[n] lp;\n  lp = rep_vector(log_unif, n);\n  for (tau in 1:n)\n    for (i in 1:n)\n      lp[tau] += poisson_lpmf(count[i] | i < tau ? lambda_1 : lambda_2);\n}\n\nmodel {\n  lambda_1 ~ exponential(alpha);\n  lambda_2 ~ exponential(alpha);\n  target += log_sum_exp(lp);\n}\n\ngenerated quantities {\n  int<lower=1,upper=n> tau;\n  vector<lower=0>[n] expected;\n  // posterior of discrete change point\n  tau = categorical_logit_rng(lp);\n  // predictions for each day\n  for (day in 1:n)\n    expected[day] = day < tau ? lambda_1 : lambda_2;\n}\n\n\nNow for sampling from the distribution and obtaining the marginal distributions:\n\n\nmod <- cmdstanr::cmdstan_model(\"models/ch1-mod.stan\")\n\nfit <- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000,\n  iter_sampling = 3000\n)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 11.7 seconds.\nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 13.3 seconds.\nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 13.5 seconds.\nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 14.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 13.2 seconds.\nTotal execution time: 14.7 seconds.\n\nfit$summary()\n\n\n# A tibble: 152 x 10\n   variable   mean median    sd   mad     q5    q95  rhat ess_bulk\n   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>\n 1 lp__     -481.  -481.  1.13  0.715 -483.  -480.   1.00    4770.\n 2 lambda_1   17.8   17.7 0.678 0.646   16.7   18.8  1.00    6623.\n 3 lambda_2   22.7   22.7 0.981 0.889   21.2   24.2  1.00    8000.\n 4 lp[1]    -512.  -511.  9.47  8.46  -529.  -500.   1.00    9227.\n 5 lp[2]    -511.  -509.  9.16  8.10  -526.  -499.   1.00    9025.\n 6 lp[3]    -512.  -510.  9.13  8.14  -527.  -500.   1.00    9516.\n 7 lp[4]    -509.  -507.  8.71  7.58  -524.  -498.   1.00    9131.\n 8 lp[5]    -510.  -508.  8.67  7.61  -524.  -499.   1.00    9471.\n 9 lp[6]    -506.  -505.  8.25  7.01  -520.  -496.   1.00    9095.\n10 lp[7]    -510.  -509.  8.48  7.46  -525.  -499.   1.00    9637.\n# … with 142 more rows, and 1 more variable: ess_tail <dbl>\n\n\n\n# https://mpopov.com/blog/2020/09/07/pivoting-posteriors/\ntidy_draws.CmdStanMCMC <- function(model, ...) {\n  return(as_draws_df(model$draws()))\n}\n\n\n\nLet’s visualize the marginal distributions of \\(\\lambda_1\\) and \\(\\lambda_2\\):\n\n\ndraws_df <- as_draws_df(fit$draws(c(\"lambda_1\", \"lambda_2\", \"tau\")))\n\ndraws_df %>% \n  gather_draws(lambda_1, lambda_2) %>%\n  ggplot(aes(y = .variable, x = .value)) +\n  stat_dotsinterval(quantiles = 100) +\n  labs(\n    title = TeX(\"Posterior Distributions of $\\\\lambda_1$ and $\\\\lambda_2$\"),\n    y = NULL\n  )\n\n\n\n\nReviewing when the change point occurred:\n\n\ndraws_df %>% \n  gather_draws(tau) %>% \n  ggplot(aes(x = .value)) +\n  geom_bar(aes(y = ..count../sum(..count..))) +\n  scale_x_continuous(breaks = scales::pretty_breaks(10), limits = c(40, 50)) +\n  labs(\n    title = TeX(\"Posterior Distribution of $\\\\tau$\"),\n    y = \"probability\",\n    x = TeX(\"$\\\\tau$ (in days)\")\n  ) \n\n\n\n\n\nOur analysis also returned a distribution for \\(\\tau\\). Its posterior distribution looks a little different from the other two because it is a discrete random variable, so it doesn’t assign probabilities to intervals. We can see that near day 46, there was a 50% chance that the user’s behaviour changed. Had no change occurred, or had the change been gradual over time, the posterior distribution of \\(\\tau\\) would have been more spread out, reflecting that many days were plausible candidates for \\(\\tau\\). By contrast, in the actual results we see that only three or four days make any sense as potential transition points.\n\n1.4.4 What Good Are Samples from the Posterior, Anyways?\nNow for calculating the expected values:\n\n\npredictions <- fit$draws(\"expected\") %>% \n  as_draws_df() %>% \n  spread_draws(expected[day])\n\npredictions\n\n\n# A tibble: 888,000 x 5\n# Groups:   day [74]\n     day expected .chain .iteration .draw\n   <int>    <dbl>  <int>      <int> <int>\n 1     1     18.8      1          1     1\n 2     1     16.8      1          2     2\n 3     1     17.9      1          3     3\n 4     1     18.3      1          4     4\n 5     1     17.3      1          5     5\n 6     1     17.8      1          6     6\n 7     1     17.8      1          7     7\n 8     1     17.7      1          8     8\n 9     1     16.4      1          9     9\n10     1     17.9      1         10    10\n# … with 887,990 more rows\n\nLet’s visualize these predictions now including the uncertainty around our \\(\\lambda\\) parameters:\n\n\npredictions %>% \n  mean_qi(.width = c(.99, .95, .8)) %>% \n  ggplot(aes(x = day)) +\n  geom_col(data = data, aes(y = count)) +\n  geom_lineribbon(aes(y = expected, ymin = .lower, ymax = .upper), color = \"#08519C\") +\n  labs(\n    title = \"Expected number of text-messages received\",\n    x = \"Time (days)\",\n    y = \"Expected # text-messages\"\n  ) +\n  scale_fill_brewer() \n\n\n\n\n1.6 Appendix\n1.6.1 Determining Statistically if the Two \\(\\lambda\\)s Are Indeed Different?\nNow for reviewing the difference between these two \\(\\lambda\\)’s. In the book he answers this question with explicit probabilities but we can also represent the same question with a data visualization. This shows us that the probability that these \\(\\lambda\\)s differ by at least 5 times is 50%.\n\n\ndraws_df %>% \n  gather_draws(lambda_1, lambda_2) %>%\n  compare_levels(.value, by = .variable) %>% \n  ggplot(aes(y = .variable, x = .value, fill = stat(abs(x) < 5))) +\n  stat_halfeye() +\n  geom_vline(xintercept = c(-5,5), linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"gray80\", \"skyblue\")) +\n  labs(\n    title = \"Comparing our parameters\",\n    y = NULL\n  )\n\n\n\n\n\n\n\n",
    "preview": "bayes-hackr/2021-01-08-bayes-hackr-ch1/bayes-hackr-ch1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-08T19:45:31-06:00",
    "input_file": {}
  },
  {
    "path": "bayes-hackr/2021-01-08-bayes-hackr-ch2/",
    "title": "Chapter 2",
    "description": "Reproducing Chapter 2 of Bayesian Methods for Hackers in R + Stan",
    "author": [
      {
        "name": "J. Duncan",
        "url": {}
      }
    ],
    "date": "2021-01-08",
    "categories": [],
    "contents": "\n2.2 Modeling Approaches\n2.2.10 Example: Challenger Space Shuttle Disaster\n\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(latex2exp)\nlibrary(modelr)\n\nurl <- \"https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv\"\n\ndata <- read_csv(url, col_types = \"cdi\") %>% \n  janitor::clean_names() %>% \n  mutate(\n    date = if_else(\n      nchar(date) >= 10,\n      parse_date(date, format = \"%m/%d/%Y\"),\n      parse_date(date, format = \"%m/%d/%y\")\n      )\n    ) %>% \n  drop_na()\n\n\n\nLet’s visualize this data to make sure all is well:\n\n\ntheme_set(theme_tidybayes())\n\ndata %>% \n  ggplot(aes(x=temperature, y=damage_incident)) + \n  geom_point(alpha = 0.5) +\n  labs(\n    title = \"Defects of the Space Shuttle O-Rings vs temperature\",\n    x = \"Outside temperautre (Farenheit)\",\n    y = \"Damage Incident\"\n  ) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 1)) +\n  xlim(50, 85)\n\n\n\n\nhttps://jrnold.github.io/bayesian_notes/binomial-models.html\n\n\nmap_df(\n  list(-1, -3, 5),\n  ~ tibble(\n    x = seq(-4, 4, length.out = 101),\n    y = plogis(x * .x),\n    Parameter = paste0(\"Beta: \", .x)\n    )\n  ) %>%\n  ggplot(aes(x = x, y = y, colour = Parameter)) +\n  geom_line() \n\n\n\n\n2.2.11 The Normal Distribution\n\n\ndata_list <- data %>% \n  rename(damage = damage_incident) %>% \n  compose_data()\n\n\n\n\n\nmod <- cmdstanr::cmdstan_model(\"models/ch2-mod.stan\")\nmod$print()\n\n\ndata {\n  int<lower=0> n;\n  vector[n] temperature;\n  int<lower=0,upper=1> damage[n];\n}\n\nparameters {\n  real alpha;\n  real beta;\n}\n\nmodel {\n  alpha ~ normal(0, sqrt(1000));\n  beta ~ normal(0, sqrt(1000));\n  damage ~ bernoulli_logit(beta * temperature + alpha);\n}\n\ngenerated quantities {\n  vector[n] yrep;\n  for (i in 1:n){\n    yrep[i] = bernoulli_logit_rng(beta * temperature[i] + alpha);\n  }\n}\n\nfit <- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000,\n  iter_sampling = 3000\n)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1001 / 4000 [ 25%]  (Sampling) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.8 seconds.\nChain 2 finished in 0.9 seconds.\nChain 3 finished in 0.9 seconds.\nChain 4 finished in 0.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.9 seconds.\nTotal execution time: 1.1 seconds.\n\nfit$summary()\n\n\n# A tibble: 26 x 10\n   variable    mean  median    sd   mad      q5     q95  rhat ess_bulk\n   <chr>      <dbl>   <dbl> <dbl> <dbl>   <dbl>   <dbl> <dbl>    <dbl>\n 1 lp__     -11.3   -11.0   1.02  0.743 -13.3   -10.3    1.00    2616.\n 2 alpha     17.4    16.6   7.77  7.66    6.16   31.2    1.00    1526.\n 3 beta      -0.267  -0.256 0.114 0.112  -0.469  -0.104  1.00    1536.\n 4 yrep[1]    0.448   0     0.497 0       0       1      1.00   11428.\n 5 yrep[2]    0.232   0     0.422 0       0       1      1.00   11617.\n 6 yrep[3]    0.270   0     0.444 0       0       1      1.00   11845.\n 7 yrep[4]    0.327   0     0.469 0       0       1      1.00   11921.\n 8 yrep[5]    0.376   0     0.484 0       0       1      1.00   11480.\n 9 yrep[6]    0.156   0     0.363 0       0       1      1.00   11547.\n10 yrep[7]    0.128   0     0.334 0       0       1      1.00   10240.\n# … with 16 more rows, and 1 more variable: ess_tail <dbl>\n\nhttps://mpopov.com/blog/2020/09/07/pivoting-posteriors/\n\n\ntidy_draws.CmdStanMCMC <- function(model, ...) {\n  return(as_draws_df(model$draws()))\n}\n\n\n\n\n\nfit %>% \n  gather_draws(alpha, beta) %>% \n  ggplot(aes(x = .value)) +\n  geom_histogram() +\n  facet_wrap(~.variable, scales = \"free\", ncol = 1) +\n  labs(\n    title = TeX(\"Posterior Distributions of $\\\\alpha$ and $\\\\beta$\"),\n    y = NULL\n  )\n\n\n\n\nUsing comments from discourse:\n\nExtract posterior parameter samples into R and do prediction in R with new-data-for-prediction.\n\n\n\npost_df <- spread_draws(fit, alpha, beta)\nt <- seq(min(data$temperature) - 5, max(data$temperature) + 5, length.out = 50)\n\npred_df <- tibble(temperature = t) %>% \n  mutate(nest(post_df)) %>% # nest all posterior parameters samples\n  mutate(\n    data = map2(\n      data, \n      temperature, \n      ~ mutate(.x, pred = plogis(alpha + beta * .y)) # predict\n    )\n  ) %>% \n  unnest(data)\n\n\n\nNow for the visualizations. First let’s look at realizations and then will summarize all of the realizations:\n\n\npred_df %>% \n  group_by(temperature) %>% \n  sample_draws(100) %>% \n  ggplot(aes(x = temperature)) +\n  geom_line(aes(y = pred, group = .draw), alpha = 0.3) + \n  geom_point(data = data, aes(y = damage_incident), alpha = 0.3) +\n  labs(\n    title = \"Posterior probability estimates given temp; realizations\",\n    y = \"probability estimate\"\n  )\n\n\n\n\nNow for all of the realizations:\n\n\npred_df %>% \n  group_by(temperature) %>% \n  median_hdci(pred, .width = c(0.95, 0.8)) %>% \n  ggplot(aes(x = temperature)) +\n  geom_lineribbon(aes(y = pred, ymin = .lower, ymax = .upper), color = \"#08519C\") +\n  geom_point(data = data, aes(y = damage_incident), alpha = 0.5) +\n  scale_fill_brewer() +\n  labs(\n    title = \"Posterior probability estimates given temp\",\n    y = \"probability estimate\"\n  )\n\n\n\n\n2.2.12 What about the day of the Challenger disaster?\n\nOn the day of the Challenger disaster, the outside temperature was 31 degrees Fahrenheit. What is the posterior distribution of a defect occurring, given this temperature? The distribution is plotted below. It looks almost guaranteed that the Challenger was going to be subject to defective O-rings.\n\n\n\nprob_31 <- post_df %>% \n  mutate(pred = plogis(alpha + beta * 31))\n\nprob_31 %>% \n  ggplot(aes(x = pred)) +\n  stat_dots(quantiles = 25) +\n  labs(\n    title = TeX(\"Posterior distribution of probability of defect, given $t = 31$\"),\n    x = \"probability of defect occurring in O-ring\",\n    y = NULL\n  )\n\n\n\n\n2.3 Is Our Model Appropriate?\n2.3.1 Separation Plots\nWhere expected number of defects for the Bernoulli random variable is given as:\n\\[\nE[S] = \\sum_{i=0}^N E[X_i] = \\sum_{i=0}^N p_i\n\\] Preparing our data for the separation plot:\n\n\nsep_data <- fit %>% \n  spread_draws(yrep[day]) %>% \n  mean_hdci(yrep) %>% \n  left_join(mutate(data, day = row_number())) %>% \n  arrange(yrep) %>% \n  mutate(idx = row_number())\n\nexp_def <- sep_data %>% \n  summarize(exp_def = sum(yrep)) %>% \n  pull()\n\n\n\nNow let’s review the separation plot:\n\n\nsep_data %>% \n  ggplot(aes(x=idx)) +\n  geom_col(aes(y=damage_incident), alpha = 0.3) +\n  geom_step(aes(y = yrep)) +\n  geom_vline(xintercept = 23 - exp_def, linetype = \"dotted\") +\n  labs(\n    title = \"Separation Plot\",\n    subtitle = paste0(\"with \", round(exp_def, 1), \" expected defects\"),\n    y = NULL,\n    x = NULL\n  )\n\n\n\n\n\nThe snaking-line is the sorted probabilities, blue gray bars denote defects, and empty space denote non-defects. As the probability rises, we see more and more defects occur. On the right hand side, the plot suggests that as the posterior probability is large (line close to 1), then more defects are realized. This is good behaviour. Ideally, all the blue gray bars should be close to the right-hand side, and deviations from this reflect missed predictions.\n\n\n\n\n",
    "preview": "bayes-hackr/2021-01-08-bayes-hackr-ch2/bayes-hackr-ch2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-08T19:46:33-06:00",
    "input_file": {}
  },
  {
    "path": "bayes-hackr/2021-01-08-bayes-hackr-ch3/",
    "title": "Chapter 3",
    "description": "Reproducing Chapter 3 of Bayesian Methods for Hackers in R + Stan",
    "author": [
      {
        "name": "J. Duncan",
        "url": {}
      }
    ],
    "date": "2021-01-08",
    "categories": [],
    "contents": "\n3.1 The Bayesian Landscape\n3.1.4 Example: Unsupervised Clustering Using a Mixture Model\n\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(tidybayes)\nlibrary(rstan)\nlibrary(patchwork)\nlibrary(distributional)\n\nurl <- \"https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/data/mixture_data.csv\"\n\ndata <- read_csv(url, col_names = FALSE) %>% \n  rename(x = X1)\n\n\n\nLet’s visualize this data to make sure all is well:\n\n\ntheme_set(theme_tidybayes())\n\ndata %>% \n  ggplot(aes(x=x)) + \n  stat_histinterval() +\n  labs(\n    title = \"Mixture Data\",\n    x = \"Value\",\n    y = NULL\n  ) \n\n\n\n\n\n\ndata_list <- list(\n  N = 300,\n  K = 2, # number of clusters\n  y = data$x\n)\n\n\n\nhttps://mc-stan.org/docs/2_25/stan-users-guide/summing-out-the-responsibility-parameter.html\n\n\nmod <- cmdstanr::cmdstan_model(\"models/ch3-mod.stan\")\nmod$print()\n\n\ndata {\n  int<lower=1> K;          // number of mixture components\n  int<lower=1> N;          // number of data points\n  real y[N];               // observations\n}\n\nparameters {\n  simplex[K] theta;          // mixing proportions\n  ordered[K] mu;             // locations of mixture components\n  vector<lower=0>[K] sigma;  // scales of mixture components\n}\n\nmodel {\n  vector[K] log_theta = log(theta);  // cache log calculation\n  sigma ~ uniform(0, 100);\n  mu[1] ~ normal(120, 10);\n  mu[2] ~ normal(190, 10);\n  \n  for (n in 1:N) {\n    vector[K] lps = log_theta;\n    for (k in 1:K)\n      lps[k] += normal_lpdf(y[n] | mu[k], sigma[k]);\n    target += log_sum_exp(lps);\n  }\n}\n\ngenerated quantities {\n  vector[N] yrep;\n  for (i in 1:N){\n    vector[K] log_theta = log(theta);\n    yrep[i] = (normal_lpdf(y[i] | mu[1], sigma[1]) + log_theta[1]) >\n    (normal_lpdf(y[i] | mu[2], sigma[2]) + log_theta[2]);\n  }\n}\n\nfit <- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 2,\n  parallel_chains = 2,\n  refresh = 1000,\n  iter_warmup = 4000,\n  iter_sampling = 8000\n)\n\n\nRunning MCMC with 2 parallel chains...\n\nChain 1 Iteration:     1 / 12000 [  0%]  (Warmup) \nChain 2 Iteration:     1 / 12000 [  0%]  (Warmup) \nChain 2 Iteration:  1000 / 12000 [  8%]  (Warmup) \nChain 1 Iteration:  1000 / 12000 [  8%]  (Warmup) \nChain 1 Iteration:  2000 / 12000 [ 16%]  (Warmup) \nChain 2 Iteration:  2000 / 12000 [ 16%]  (Warmup) \nChain 1 Iteration:  3000 / 12000 [ 25%]  (Warmup) \nChain 1 Iteration:  4000 / 12000 [ 33%]  (Warmup) \nChain 1 Iteration:  4001 / 12000 [ 33%]  (Sampling) \nChain 1 Iteration:  5000 / 12000 [ 41%]  (Sampling) \nChain 1 Iteration:  6000 / 12000 [ 50%]  (Sampling) \nChain 1 Iteration:  7000 / 12000 [ 58%]  (Sampling) \nChain 1 Iteration:  8000 / 12000 [ 66%]  (Sampling) \nChain 1 Iteration:  9000 / 12000 [ 75%]  (Sampling) \nChain 1 Iteration: 10000 / 12000 [ 83%]  (Sampling) \nChain 1 Iteration: 11000 / 12000 [ 91%]  (Sampling) \nChain 1 Iteration: 12000 / 12000 [100%]  (Sampling) \nChain 1 finished in 34.0 seconds.\nChain 2 Iteration:  3000 / 12000 [ 25%]  (Warmup) \nChain 2 Iteration:  4000 / 12000 [ 33%]  (Warmup) \nChain 2 Iteration:  4001 / 12000 [ 33%]  (Sampling) \nChain 2 Iteration:  5000 / 12000 [ 41%]  (Sampling) \nChain 2 Iteration:  6000 / 12000 [ 50%]  (Sampling) \nChain 2 Iteration:  7000 / 12000 [ 58%]  (Sampling) \nChain 2 Iteration:  8000 / 12000 [ 66%]  (Sampling) \nChain 2 Iteration:  9000 / 12000 [ 75%]  (Sampling) \nChain 2 Iteration: 10000 / 12000 [ 83%]  (Sampling) \nChain 2 Iteration: 11000 / 12000 [ 91%]  (Sampling) \nChain 2 Iteration: 12000 / 12000 [100%]  (Sampling) \nChain 2 finished in 82.3 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 58.2 seconds.\nTotal execution time: 82.4 seconds.\n\nfit$summary()\n\n\n# A tibble: 307 x 10\n   variable     mean   median     sd    mad       q5      q95  rhat\n   <chr>       <dbl>    <dbl>  <dbl>  <dbl>    <dbl>    <dbl> <dbl>\n 1 lp__     -1.54e+3 -1.54e+3 1.67   1.47   -1.54e+3 -1.53e+3  1.00\n 2 theta[1]  3.77e-1  3.73e-1 0.0507 0.0475  3.01e-1  4.65e-1  1.00\n 3 theta[2]  6.23e-1  6.27e-1 0.0507 0.0475  5.35e-1  6.99e-1  1.00\n 4 mu[1]     1.20e+2  1.20e+2 5.71   5.31    1.12e+2  1.30e+2  1.00\n 5 mu[2]     2.00e+2  2.00e+2 2.62   2.64    1.95e+2  2.04e+2  1.00\n 6 sigma[1]  3.02e+1  2.98e+1 4.07   3.80    2.44e+1  3.77e+1  1.00\n 7 sigma[2]  2.28e+1  2.28e+1 1.91   1.87    1.99e+1  2.61e+1  1.00\n 8 yrep[1]   1.00e+0  1.00e+0 0      0       1.00e+0  1.00e+0 NA   \n 9 yrep[2]   8.40e-1  1.00e+0 0.367  0       0.       1.00e+0  1.00\n10 yrep[3]   2.75e-3  0.      0.0524 0       0.       0.       1.00\n# … with 297 more rows, and 2 more variables: ess_bulk <dbl>,\n#   ess_tail <dbl>\n\nposterior <- fit$output_files() %>% \n  read_stan_csv()\n\n\n\n\n\np1 <- posterior %>% \n  mcmc_trace(pars = c(\"mu[1]\", \"mu[2]\")) +\n  labs(\n    title = \"Traces of center for each cluster\"\n  )\n\np2 <- posterior %>% \n  mcmc_trace(pars = c(\"sigma[1]\", \"sigma[2]\")) +\n  labs(\n    title = \"Traces of standard deviation of each cluster\"\n  )\n\np3 <- posterior %>% \n  mcmc_trace(pars = c(\"theta[1]\")) +\n  labs(\n    title = \"Frequency of assignment to cluster 1\"\n  )\n\np1 / p2 / p3 \n\n\n\n\n\nNotice the following characteristics:\nThe traces converges, not to a single point, but to a distribution of possible points. This is convergence in an MCMC algorithm.\nInference using the first few thousand points is a bad idea, as they are unrelated to the final distribution we are interested in. Thus is it a good idea to discard those samples before using the samples for inference. We call this period before converge the burn-in period.\nThe traces appear as a random “walk” around the space, that is, the paths exhibit correlation with previous positions. This is both good and bad. We will always have correlation between current positions and the previous positions, but too much of it means we are not exploring the space well. This will be detailed in the Diagnostics section later in this chapter.\n\nCluster Investigation\n\n\n# https://mpopov.com/blog/2020/09/07/pivoting-posteriors/\ntidy_draws.CmdStanMCMC <- function(model, ...) {\n  return(as_draws_df(model$draws()))\n}\n\n\n\n\n\nfit %>% \n  gather_draws(mu[cluster], sigma[cluster]) %>% \n  mutate(cluster = paste0(\"Cluster \", cluster)) %>% \n  ggplot(aes(.value)) +\n  stat_histinterval() +\n  facet_wrap(vars(cluster, .variable), ncol = 2, scales = \"free\") +\n  labs(\n    title = \"Posterior of center and standard deviation of clusters 1 & 2\",\n    y = NULL,\n    x = NULL\n  )\n\n\n\n\n\nOne quick and dirty way (which has nice theoretical properties we will see in Chapter 5), is to use the mean of the posterior distributions. Below we overlay the Normal density functions, using the mean of the posterior distributions as the chosen parameters, with our observed data:\n\n\n\ndistdata <- fit %>% \n  spread_draws(mu[cluster], sigma[cluster], theta[cluster]) %>% \n  mean_hdci() %>% \n  mutate(\n    cluster = as_factor(cluster),\n    nest(tibble(x = seq(20, 300, length.out = 500)))\n    ) %>% \n  mutate(\n    data = pmap(\n      list(data, mu, sigma, theta),\n      function(data, mu, sigma, theta) \n        mutate(data, dens = theta * dnorm(x, mean = mu, sd = sigma))\n      )\n  ) %>% \n  unnest() \n\ndistdata %>% \n  ggplot(aes(x = x)) +\n  geom_histogram(\n    data = data, \n    aes(y = stat(density)), color = \"black\", fill = \"white\"\n    ) +\n  geom_ribbon(aes(ymin = 0, ymax = dens, fill = cluster), alpha = 0.5) +\n  labs(\n    title = \"Visualizing Clusters using posterior-mean parameters\",\n    y = NULL,\n    x = NULL\n  ) +\n  scale_fill_viridis_d()\n\n\n\n\nReturning to Clustering: Prediction\n\nWe can try a less precise, but much quicker method. We will use Bayes’ Theorem for this. As you’ll recall, Bayes’ Theorem looks like:\n\n\\[\nP(A|X) = \\frac{P(X|A)P(A)}{P(X)} \n\\]\n\nFor a particular sample set of parameters for our posterior distribution, (\\(\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,\\theta\\)), we are interested in asking, “Is the probability that \\(x\\) is in the cluster 1 greater than the probability it is in cluster 0?” where the probability is dependent on the chosen parameters:\n\n\\[\n\\frac{P(x = ?|L_x = 1)P(L_x=1)}{P(x=?)} > \\frac{P(x = ?|L_x = 2)P(L_x=2)}{P(x=?)} \n\\]\n\nSince the denominators are equal, they can be ignored:\n\n\\[\nP(x = ?|L_x = 1)P(L_x=1) > P(x = ?|L_x = 2)P(L_x=2)\n\\] This is what you’ll see in the generated quantities block but instead on the log space. There is likely a more precise way to estimate the probability of the data points belonging to each cluster. I think it would be similar to what was done in the Chapter 1 model for the change point detection.\n\n\ndataorig <- data %>% \n  mutate(i = row_number())\n\nprobassign <- fit %>% \n  gather_draws(yrep[i]) %>% \n  mean_hdci() %>% \n  left_join(dataorig)\n\nprobassign %>% \n  ggplot(aes(x = x, y = .value)) +\n  geom_point(aes(color = .value)) +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Probability of data point belonging to cluster 0\",\n    y = \"probability\",\n    x = \"value of data point\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n",
    "preview": "bayes-hackr/2021-01-08-bayes-hackr-ch3/bayes-hackr-ch3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-01-08T20:07:23-06:00",
    "input_file": {}
  }
]
