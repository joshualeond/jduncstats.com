[
  {
    "path": "posts/2021-11-02-a-bayesian-approach-to-the-t-test/",
    "title": "A Bayesian Approach to the t-test",
    "description": "Comparing two methods in a chemical manufacturing plant in an attempt to understand if one has a **significantly better** yield than the other",
    "author": [
      {
        "name": "J. Duncan",
        "url": "https://jduncstats.com/posts/2021-11-02-a-bayesian-approach-to-the-t-test/"
      }
    ],
    "date": "2021-11-02",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nThe original\nquestion and the \\(t\\)-test\nVisualizing the data\nThe \\(t\\)-test results\nVisualizing the \\(t\\)-test results\n\nA Bayesian approach\nChoosing our priors\nThinking in\ndistributions\nPrior for the variance\nOur probability model\nMCMC and Metropolis\nsampling\nBurn-in\nSamples from the\njoint distribution\nComparing the means\nusing samples\n\nAnalysis results and\nimprovements\nSummary of results\nPotential Improvements\n\n\nBased on the book by Box, Hunter, and Hunter: Statistics for\nExperimenters\nChapter 3: Comparing Two Entities: Reference Distributions, Tests,\nand Confidence Intervals\nIntroduction\nIn Statistics for Experimenters we are shown an example of\ncomparing two methods in a chemical manufacturing plant in an attempt to\nunderstand if one method has a significantly better\nyield than the other. I will present the data and the original model and\nthen we’ll model this comparison using Bayesian methods.\nRoadmap\nThe original question and the frequentist approach to comparing\nmeans\nA Bayesian approach to the \\(t\\)-test\nSummary of our analysis results and potential improvements\nThe original question and\nthe \\(t\\)-test\n\nAn experiment was performed on a manufacturing plant by making in\nsequence \\(10\\) batches of a\nchemical using a standard production method \\((A)\\) followed by \\(10\\) batches using a modified method \\((B)\\). What evidence do the data provide\nthat method \\(B\\) gives higher yields\nthan method \\(A\\)?\n\nVisualizing the data\nusing DataFrames, CairoMakie, Statistics\n\n# prepare dataframe\ntime = 1:1:20\nmethod = repeat([\"A\", \"B\"], inner = 10)\nyield = [89.7, 81.4, 84.5, 84.8, 87.3, 79.7, 85.1, 81.7, 83.7, 84.5, 84.7, 86.1, \n83.2, 91.9, 86.3, 79.3, 82.6, 89.1, 83.7, 88.5]\n\ndata = DataFrame(time = time, method = method, yield = yield)\nfirst(data, 5)\ntime\nmethod\nyield\n1\nA\n89.7\n2\nA\n81.4\n3\nA\n84.5\n4\nA\n84.8\n5\nA\n87.3\nThe averages are shown for both of the methods so we have a crude\ndifference in yield of around \\(1.3\\)\nbetween these two methods. But is this average difference due to random\nchance?\n\\[\n\\bar y_A = 84.24, \\bar y_B = 85.54\n\\]\n# plot attributes\nCairoMakie.activate!(type = \"svg\")\nset_theme!(theme_minimal())\nfig = Figure(resolution = (650, 300))\nax = fig[1, 1] = Axis(fig, xlabel = \"time\", ylabel = \"yield\", xticks = 1:1:20)\n\n# calculate the mean for each group\ndatagroup = groupby(data, :method)\ndatamean = transform(datagroup, :yield => mean)\n\nmethoda = datamean[datamean.method .== \"A\",:]\nmethodb = datamean[datamean.method .== \"B\",:]\n\n# plot it out\nscatter!(methoda.time, methoda.yield, label = \"Process A\")\nlines!(methoda.time, methoda.yield_mean)\nscatter!(methodb.time, methodb.yield, label = \"Process B\")\nlines!(methodb.time, methodb.yield_mean)\n\nfig[2,1] = Legend(fig, ax, framevisible = false, orientation = :horizontal)\nfig\n\nThe \\(t\\)-test results\nWhere the sample averages for each method are \\(\\bar y_A\\) and \\(\\bar y_B\\), the population difference is\ngiven as \\(\\delta = \\eta_B - \\eta_A\\),\nthe pooled estimate of \\(\\sigma\\) is\ngiven as \\(s\\), and the number of\nobservations for each method are \\(n_A\\) and \\(n_B\\). Then we can calculate the \\(t\\)-statistic:\n\\[\nt_0 = \\frac{(\\bar y_B - \\bar y_A) - \\delta_0}{s \\sqrt{1 / n_B + 1 /\nn_A}}\n\\]\nFor the null hypothesis we have \\(\\delta_0 = 0, t_0 = 1.30/1.47 = 0.88\\) with\n\\(\\nu = 18\\) degrees of freedom. Then\nwe want the following for our significance level:\n\\[\n\\Pr(t \\ge 0.88) = 19.5\\%\n\\]\n\nThis reference distribution assumes the random sampling model and the\nobserved difference then has a significance probability of \\(19.5\\%\\) which would provide little\nevidence against the hypothesis that \\(\\eta_B\n- \\eta_A = 0\\).\n\nVisualizing the \\(t\\)-test results\nScaling a \\(t\\)-distribution by the\nstandard error of the difference between \\(\\bar y_B\\) and \\(\\bar y_A\\) and looking at the probability\ndensity that exists above our estimated mean difference of \\(1.3\\) we can visualize the results of our\ntest. Seeing approximately \\(19.5 \\%\\)\nof the probability mass above this difference:\nusing Distributions\n\n# number of obs\nn_a = nrow(methoda)\nn_b = nrow(methodb)\n\n# average yield per method\navg_a = mean(methoda.yield)\navg_b = mean(methodb.yield)\ndiff_ab = avg_b - avg_a\n\n# sample variance\ns2_a = sum((methoda.yield .- avg_a).^2)\ns2_b = sum((methodb.yield .- avg_b).^2)\n\n# degrees of freedom\nnu = n_a + n_b - 2\n\n# pooled estimate of σ^2\npools2 = (s2_a + s2_b) / nu\n\n# estimated standard error of y_B - y_A\nstd_err = sqrt(pools2/5)\n\n# t-statistic\ntstat = diff_ab / std_err\n\n# significance level\nsig_level = 1 - cdf(TDist(nu), tstat)\n\n# plotting attributes\nfig2 = Figure(resolution = (600, 300))\nax2 = Axis(fig2[1, 1], title = \"Scaled t-distribution\")\n\n# A scaled t-distribution by the standard error of y_B - y_A\nscaled_dist = LocationScale(0.0, std_err, TDist(nu))\n\n# get density line and clipped values\nxs2 = -5:0.001:5\npost_range = pdf.(scaled_dist, xs2)\npost_clip = [x < diff_ab ? 0 : pdf(scaled_dist, x) for x in xs2] \n\n# plot\nlines!(xs2, post_range)\nband!(xs2, 0, post_clip)\nvlines!(ax2, [diff_ab], color = :black, linestyle = :dash)\nhideydecorations!(ax2)\nfig2\n\nA Bayesian approach\nChoosing our priors\nChoosing our priors is the first step in this approach. Since we’ll\nbe modeling the average yield for each method I will begin by\nconsidering how yield is defined\nand what that can tell us about its order of magnitude.\nPercent yield is a measurement of how successful a chemical reaction\nhas been:\n\n…the theoretical yield, the maximum amount of\nproduct that can be formed from the given amounts of reactants. The\nactual yield is the amount of product that is actually\nformed when the reaction is carried out in the laboratory. The\npercent yield is the ratio of the actual yield to the\ntheoretical yield, expressed as a percentage. \\[\n\\text{Percent Yield} = \\frac{\\text{Actual Yield}}{\\text{Theoretical\nYield}} \\times 100\\%\n\\]\n\nThinking in distributions\nConsidering that yield here is a percentage I’d think a\ndecent prior probability distribution for the average yield would be a\nBeta distribution:\n# plotting attributes\nfig3 = Figure(resolution = (550, 250))\nax3 = Axis(fig3[1, 1])\n\nbeta_dist = Beta(1, 1)\nbeta_dist2 = Beta(2, 2)\nbeta_dist3 = Beta(2, 5)\n    \n# get density line and clipped values\nxs3 = 0.:0.001:1\npost_range2 = pdf.(beta_dist, xs3)\npost_range3 = pdf.(beta_dist2, xs3)\npost_range4 = pdf.(beta_dist3, xs3)\n    \n# plot\nlines!(xs3, post_range2, label = \"Beta(1, 1)\")\nlines!(xs3, post_range3, label = \"Beta(2, 2)\")\nlines!(xs3, post_range4, label = \"Beta(2, 5)\")\nhideydecorations!(ax3)\naxislegend()\nfig3\n\nSince percent yield could reasonably be anywhere between \\(0\\) and \\(1\\) but with those outcomes being very\nunlikely I think a Beta(2, 2) is an adequate weakly informative\nprior.\nPrior for the variance\nWe will be modeling the variance as being pooled between both\nmethods: we will have two means but one shared variance. The variance\nmust be positive and since our means are scaled between \\([0,1]\\) then the standard deviation should\nbe close to this range.\n# plotting attributes\nfig4 = Figure(resolution = (550, 250))\nax4 = Axis(fig4[1, 1])\n\nexp_dist = Exponential(.50)\nexp_dist2 = Exponential(.25)\nexp_dist3 = Exponential(.10)\n    \n# get density line and clipped values\nxs4 = 0.:0.001:1.\nprior_range = pdf.(exp_dist, xs4)\nprior_range2 = pdf.(exp_dist2, xs4)\nprior_range3 = pdf.(exp_dist3, xs4)\n    \n# plot\nlines!(xs4, prior_range, label = \"Exponential(0.50)\")\nlines!(xs4, prior_range2, label = \"Exponential(0.25)\")\nlines!(xs4, prior_range3, label = \"Exponential(0.10)\")\nhideydecorations!(ax4)\naxislegend()\nfig4\n\nGiven that the Exp(0.1) distribution is almost completely under \\(1\\) it will be an adequate prior for \\(\\sigma\\).\nOur probability model\n\\(\\mu_A\\) and \\(\\mu_B\\) are the estimated population means\nfor each of the methods and they have a shared variance term \\(\\sigma^2\\). The priors for these are the\nfollowing:\n\\[\n\\begin{aligned}\n&\\mu_A \\sim \\text{Beta}(2,2) \\\\\n&\\mu_B \\sim \\text{Beta}(2,2) \\\\\n&\\sigma \\sim \\text{Exp}(0.10)\n\\end{aligned}\n\\]\nThe likelihood is broken into two parts for each of the methods:\n\\[\n\\begin{aligned}\n&y_A \\sim \\text{Normal}(\\mu_A, \\sigma) \\\\\n&y_B \\sim \\text{Normal}(\\mu_B, \\sigma)\n\\end{aligned}\n\\]\nMCMC and Metropolis sampling\nMultiplying Beta and Exponential priors with a Normal likelihood\nresults in a non-conjugate case. So in this example it may make more\nsense to utilize Markov Chain Monte Carlo (MCMC) methods to estimate our\njoint probability model.\nMarkov Chain Monte Carlo gets part of its name from the\nMarkov property:\n\nGiven the present, the future is independent of the\npast.\n\nI decided to implement the Metropolis algorithm as a\npedagogical exercise below but I’d definitely recommend using\nprobabilistic programming languages like Stan, Turing, or PyMC in practice.\nUsing the Metropolis algorithm to sample the target distribution and\nremoving burn-in samples results in the following summary for\nour parameters:\nmethoda_yield = methoda.yield / 100\nmethodb_yield = methodb.yield / 100\n\nfunction likelihood(param)\n    mu1 = param[1]\n    mu2 = param[2]\n    sd = param[3]\n    \n    singlelike1 = logpdf.(Normal(mu1, sd), methoda_yield)\n    singlelike2 = logpdf.(Normal(mu2, sd), methodb_yield)\n    singlelikelihood = vcat(singlelike1, singlelike2)\n    sumll = sum(singlelikelihood)\n    \n    return(sumll)\nend\n    \nfunction prior(param)\n    mu1 = param[1]\n    mu2 = param[2]\n    sd = param[3]\n    mu1prior = logpdf(Beta(2,2), mu1)\n    mu2prior = logpdf(Beta(2,2), mu2)\n    sdprior = logpdf(Exponential(0.10), sd)\n    return(mu1prior + mu2prior + sdprior)\nend\n    \nfunction posterior(param)\n    return likelihood(param) + prior(param)\nend\n    \nfunction proposalfunction(param)\n    # enforced positive support but not sure how other PPLs enforce this\n    return abs.(rand(MvNormal(param, [0.001 0 0; 0 0.001 0; 0 0 0.0005])))\nend\n    \nfunction run_metropolis_MCMC(startvalue, iterations)\n    chain = Array{Float64,2}(undef, iterations + 1, 3)\n    chain[1,:] = startvalue\n    \n    for i in 1:iterations\n        proposal = proposalfunction(chain[i,:])\n        probab = exp(posterior(proposal) - posterior(chain[i,:]))\n        r = min(1, probab)\n        \n        if rand(Uniform()) <= r\n            chain[i+1,:] = proposal\n        else\n            chain[i+1,:] = chain[i,:]\n        end\n    end\n    return chain\nend\n    \nstartvalue = [0.7, 0.7, 0.1]\nchain_length = 100_000\n\nusing Random:seed!\nseed!(555)\nchain = run_metropolis_MCMC(startvalue, chain_length)   \n\nburnIn = convert(Int, chain_length / 2)\nchain1 = last(chain[:,1], burnIn)\nchain2 = last(chain[:,2], burnIn)\nchain3 = last(chain[:,3], burnIn)\n\nDataFrame(\n    mu_a = mean(chain1),\n    mu_astd = std(chain1),\n    mu_b = mean(chain2),\n    mu_bstd = std(chain2),\n    sigma = mean(chain3),\n    sigma_std = std(chain3)\n)\nmu_a\nmu_astd\nmu_b\nmu_bstd\nsigma\nsigma_std\n0.841692\n0.0112674\n0.854442\n0.0113898\n0.0352205\n0.00627626\nBurn-in\nAfter utilizing MCMC to sample this model we should throw away some\nportion of our chain. A rule-of-thumb for Metropolis sampling is to only\nuse half of your total samples and consider the first half the\nburn-in (exploratory) phase and discard those samples.\nfigs = Figure(resolution = (800, 400))\nga = figs[1, 1] = GridLayout()\ngb = figs[2, 1] = GridLayout()\ngc = figs[3, 1] = GridLayout()\naxs1 = Axis(ga[1, 1])\naxs2 = Axis(gb[1, 1])\naxs3 = Axis(gc[1, 1])\n    \nlines!(axs1, 1:1:(chain_length+1), chain[:,1])\nlines!(axs2, 1:1:(chain_length+1), chain[:,2])\nlines!(axs3, 1:1:(chain_length+1), chain[:,3])\nvspan!(axs1, [0, chain_length/2], [0, 1], color = (:gray, 0.2))\nvspan!(axs2, [0, chain_length/2], [0, 1], color = (:gray, 0.2))\nvspan!(axs3, [0, chain_length/2], [0, 1], color = (:gray, 0.2))\nLabel(ga[1, 1, Top()], L\"\\mu_A\", valign = :bottom, textsize = 20)\nLabel(gb[1, 1, Top()], L\"\\mu_B\", valign = :bottom, textsize = 20)\nLabel(gc[1, 1, Top()], L\"\\sigma\", valign = :bottom, textsize = 20)\nhidexdecorations!.((axs1, axs2))\nfigs\n\nSamples from the joint\ndistribution\nHere are the marginal distributions for each of our estimated\nparameters:\nfigs2 = Figure(resolution = (1920/2, 1080/2))\nga2 = figs2[1, 1] = GridLayout()\ngb2 = figs2[2, 1] = GridLayout()\ngc2 = figs2[3, 1] = GridLayout()\naxs11 = Axis(ga2[1, 1])\naxs21 = Axis(gb2[1, 1])\naxs31 = Axis(gc2[1, 1])\n\nhist!(axs11, chain1)\nhist!(axs21, chain2)\nhist!(axs31, chain3)\nlinkxaxes!(axs11, axs21)\nxlims!(axs31, (0.0, 0.1)) \nhideydecorations!.((axs11, axs21, axs31))\nLabel(ga2[1, 1, Top()], L\"\\mu_A\", valign = :bottom, textsize = 30)\nLabel(gb2[1, 1, Top()], L\"\\mu_B\", valign = :bottom, textsize = 30)\nLabel(gc2[1, 1, Top()], L\"\\sigma\", valign = :bottom, textsize = 30)\nfigs2\n\nComparing the means using\nsamples\nNow that we have samples for the mean of both methods how can we\ncompare them? Simply by taking a difference of the samples we will get a\ndistribution comparing the two means.\nWhat’s gained from this approach is that we can now see the\nprobability of method \\(B\\) having an\nincreased yield to method \\(A\\). Seeing\nhow many samples are above \\(0\\) and\nthen taking the average of those results shows that \\(80\\%\\) of the probability mass is above\nzero.\nmeandiff = (chain2 - chain1) * 100\n# @show probgt0 = mean(meandiff .> 0) # just for checking\n\nfig5 = Figure(resolution = (550, 250))\nax5 = Axis(fig5[1, 1], title = L\"\\Pr(\\mu_B - \\mu_A \\geq 0) = 80 %\")\ndensity!(meandiff, bandwidth = 0.4, color =(:grey, 0.5), strokewidth=1)\nvlines!(ax5, [0.0], color = :black, linestyle = :dash)\nxlims!(-8, 8)\nhideydecorations!(ax5)\nfig5\n\nAnalysis results and\nimprovements\nSummary of results\nThe frequentist \\(t\\)-test approach\nresults in a non-significant value. We fail to reject our null\nhypothesis that these samples come from a single common\npopulation distribution.\nThe bayesian approach gives us an \\(80\n\\%\\) probability that method \\(B\\) improves overall yield compared to\nmethod \\(A\\).\nIn a business context I could see the bayesian result being far more\nuseful than failing to reject our null hypothesis. A company can weigh\ntheir options between method \\(B\\) and\nmethod \\(A\\). Decide on what’s more\neconomical and have some percentage to wrap their heads around about\npotential improvement.\nPotential Improvements\nThis particular example was given in the text by Box, Hunter, and\nHunter to demonstrate a case where the \\(t\\)-test may be an inappropriate\nstatistical test. The \\(t\\)-distribution may be a poor choice for a\nreference distribution. The reason why this is the case is due\nto autocorrelation and Student’s \\(t\\)\ntest assuming errors are independent and identically distributed\n(iid).\nThe exact reason behind the correlated errors was given:\n\nIn this particular process the yield was determined by measuring the\nvolume of liquid product in each successive batch, but because of the\ndesign of the system, a certain amount of product was left in the pipes\nand pumps at each determination. Consequently, if slightly less was\npumped out on a particular day, the recorded yield would be low but on\nthe following day it would tend to be high… Such an effect would\nproduce a negative correlation between successive yields such as that\nfound.\n\nMoving forward we should account for this and attempt to model the\nautocorrelation that exists between successive yields.\n\n\n\n",
    "preview": "posts/2021-11-02-a-bayesian-approach-to-the-t-test/featured.png",
    "last_modified": "2022-03-19T12:32:52-05:00",
    "input_file": "a-bayesian-approach-to-the-t-test.knit.md"
  },
  {
    "path": "posts/2019-11-02-golf-turing/",
    "title": "Model building of golf putting with Turing.jl",
    "description": "Reproducing the Gelman golf putting model with Julia PPL, Turing.jl",
    "author": [
      {
        "name": "J. Duncan",
        "url": "https://jduncstats.com/post/2019-11-02_golf-turing/"
      }
    ],
    "date": "2019-11-02",
    "categories": [],
    "contents": "\nUpdates (May 2020):\nI originally wrote this Turing.jl version of the golf putting model in late 2019. Since then there have been many updates to the Turing.jl Julia package. I’ve updated this blog post to be aligned with the latest release (v0.13.0) and have also included something new given that the latest release includes a feature for sampling from the prior distribution.\nFirst Thing (Disclaimer)\nThis blog post is based on a blog post written by the popular bayesian statistician Andrew Gelman.\nDr. Gelman’s post was written in the Stan probabilistic programming language (or PPL). Since this post, there have been a couple other blog posts translating the code to other PPLs. They are excellent and I definitely recommend checking them out. Here’s the list of links and my opinion on some of the benefits of reading each:\nDr. Gelman’s using Stan with R: See here. Read Dr. Gelman’s post first as he lays out the problem being modeled here really well. Explaining the geometry at play and how to adjust the models to incorporate the physics of the problem. It’s a great article showing the power of using a probabilistic programming language to build models.\nColin Carroll’s using the Python PPL PyMC3: You can check out his work here. I really enjoyed Colin’s post because of his prior and posterior predictive checks, data visualizations, and showing the value of these models moving past predictions.\nAdam Haber’s post using Python and Tensorflow Probability: See here. His post goes into great detail about more of the sampling and lower level details of Tensorflow Probability and MCMC.\nMy post is nothing really novel, simply a port to the Julia PPL, Turing.jl. This post shows how to specify and estimate these same models within the Julia programming language.\nGetting started and plotting\nLet’s Load the data and take a look:\nusing Turing, HTTP, CSV\n# Hide sampling progress.\nTuring.turnprogress(false);\n\nresp = HTTP.get(\"https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/golf/golf_data.txt\");\ndata = CSV.read(IOBuffer(resp.body), normalizenames = true, header = 3)\n\nx, n, y = (data[:,1], data[:,2], data[:,3])\nWe have three variables in this dataset:\nVariable\nUnits\nDescription\nx\nfeet\nThe distance of the attempted putt\nn\ncount\nThe total attempts (or trials) of putts at a chosen distance\ny\ncount\nThe total successful putts from the total attempts\nWhat we are attempting to build is a model to predict the probability of success given the distance from the hole. We need to transform this data a bit to explore the dataset visually. Let’s calculate the probabilities and the error involved. We’ll use the following formula to calculate the error bars for the putting success rate:\n\\[\n\\sqrt{\\hat{p}_j(1-\\hat{p}_j)/n_j}\n\\]\npj = y ./ n\nerror = @. sqrt((pj * (1 - pj) / n));\nNow let’s visualize the dataset:\nusing Plots\n\n# plot out the error\nscatter(\n  x, pj,\n  yerror= error,\n  legend = false,\n  ylim = (0, 1),\n  ylab = \"Probability of Success\",\n  xlab = \"Distance from hole (ft)\")\n\nLogistic Regression\nBuilding our first model, a GLM (Generalized Linear Model). We will attempt to model the probability of success in golf putting incorporating the distance as an independent (or predictor) variable.\n\\[\ny_j\\sim\\mbox{binomial}(n_j, \\mbox{logit}^{-1}(a + bx_j)),\n\\mbox{ for } j=1,\\dots, J.\n\\]\nusing StatsFuns: logistic\n\n@model golf_logistic(x,y,n,J) = begin\n  # parameters\n  a ~ Normal(0, 1)\n  b ~ Normal(0, 1)\n\n  # model\n  for i in 1:J\n    p = logistic(a + b * x[i])\n    y[i] ~ Binomial(n[i], p)\n  end\nend\n\nchn = sample(golf_logistic(x, y, n, length(x)), NUTS(), MCMCThreads(), 4000, 4);\nThe sample method now allows for parallel sampling with either one thread per chain (MCMCThreads) or one process per chain (MCMCDistributed). Here I’m using multithreading and am specifying 4 chains should be used.\nNow that we’ve sampled the joint probability distribution. Let’s take a look at the results. I’ll create a function to show the table of results as html using the PrettyTables.jl package. Looking at the summary statistics for the posterior distribution:\nusing PrettyTables, DataFrames\n\nformatters = (v,i,j) -> (j > 1) ? round(v, digits=3) : v\n\nfunction prettystats(chains)\n  chains |>\n    x -> summarystats(x) |>\n    x -> DataFrame(x) |>\n    x -> pretty_table(x, backend = :html, formatters = formatters)\nend\n\nprettystats(chn)\nparameters\nmean\nstd\nnaive_se\nmcse\ness\nr_hat\na\n2.225\n0.057\n0.001\n0.001\n2997.531\n1.002\nb\n-0.255\n0.007\n0.0\n0.0\n3137.058\n1.002\nVisualizing the predictions:\na_post = median(chn[:a].value)\nb_post = median(chn[:b].value)\n\n# iterator for distance from hole calcs\nxrng = 1:1:21\npost_lines = [logistic(a_post + b_post * x) for x = xrng]\n\n# 50 draws from the posterior\nusing StatsBase\na_samp = StatsBase.sample(chn[:a].value, 50)\nb_samp = StatsBase.sample(chn[:b].value, 50)\n\npost_samp = [logistic(a_samp[i] + b_samp[i] * x) for x = xrng, i = 1:50]\n\nplot!(post_samp, alpha = 0.5, color = :gray) # add uncertainty samples\nplot!(post_lines, color = :black) # add median\n\nFirst Principles\nThe next step is building a more bespoke model that incorporates the physics of this problem. Dr. Gelman describes this step as follows:\n\nWe assume that the golfer is attempting to hit the ball completely straight but that many small factors interfere with this goal, so that the actual angle follows a normal distribution centered at 0 with some standard deviation σ.\nThe probability the ball goes in the hole is then the probability that the angle is less than the threshold; that is \\[\n\\mbox{Pr}\\left(|\\mbox{angle}| < \\sin^{-1}((R-r)/x)\\right) = 2\\Phi\\left(\\frac{\\sin^{-1}((R-r)/x)}{\\sigma}\\right) - 1\n\\]\nwhere Φ is the cumulative normal distribution function.\n\nAgain, for more background I would suggest reading the original post. So now we’ll define the function Phi. It’s the cumulative distribution function of the standard normal distribution.\nPhi(x) = cdf.(Normal(0, 1), x);\nNow let’s create and sample this model incorporating the angle.\n@model golf_angle(x, y, n, J, r, R) = begin\n  # transformed data\n  threshold_angle = asin.((R - r) ./ x)\n\n  # parameters\n  sigma ~ truncated(Normal(0, 1), 0, Inf)\n\n  # model\n  p = 2 * Phi(threshold_angle / sigma) .- 1\n  for i in 1:J\n    y[i] ~ Binomial(n[i], p[i])\n  end\nend\n\n# radius of ball and hole respectively\nr = (1.68 / 2) / 12\nR = (4.25 / 2) / 12\nTaking a note from Colin’s book, we can perform prior predictive checks for the geometry-based model. In Turing we’ll simply replace the sampler argument on the sample method with Prior().\nprior = sample(golf_angle(x, y, n, length(x), r, R), Prior(), 4000)\n\nangle_prior = StatsBase.sample(prior[:sigma].value, 500)\nangle_of_shot = rand.(Normal.(0, angle_prior), 1)  # radians\nangle_of_shot = getindex.(angle_of_shot) # extract array\n\ndistance = 20 # feet\n\nend_positions = [\n    distance * cos.(angle_of_shot),\n    distance * sin.(angle_of_shot)\n]\n\n# visualize\n plot(\n   [[0, i] for i in end_positions[1]],\n   [[0, i] for i in end_positions[2]],\n   labels = false,\n   legend = :topleft,\n   color = :black,\n   alpha = 0.3,\n   title = \"Prior distribution of putts from 20ft away\"\n   )\nscatter!(end_positions[1], end_positions[2], color = :black, labels = false)\nscatter!((0,0), color = :green, label = \"start\", markersize = 6)\nscatter!((20, 0), color = :red, label = \"goal\", markersize = 6)\n\nNow for the estimation of the parameters:\nchn2 = sample(golf_angle(x, y, n, length(x), r, R), NUTS(), MCMCThreads(), 4000, 4)\nchn2 = hcat(chn2, Chains(chn2[:sigma].value * 180 / π, [\"sigma_degrees\"]))\n\nprettystats(chn2)\nparameters\nmean\nstd\nnaive_se\nmcse\ness\nr_hat\nsigma\n0.027\n0.0\n0.0\n0.0\n5818.38\n1.0\nsigma_degrees\n1.527\n0.023\n0.0\n0.0\n5818.38\n1.0\nNow we can calculate predictions and see how this model compares to the logistic model. Let’s wrap the angle calculation into a function as we’ll use it frequently throughout the post.\nprob_angle(threshold, sigma) = 2 * Phi(threshold / sigma) .- 1\nCalculate and visualize predictions.\n# calculate predictions\npost_sigma = median(chn2[:sigma].value)\nthreshold_angle = [asin((R - r) / x) for x = xrng]\ngeom_lines = prob_angle(threshold_angle, post_sigma)\n\nscatter(\n  x, pj,\n  yerror= error,\n  label = \"\",\n  ylim = (0, 1),\n  ylab = \"Probability of Success\",\n  xlab = \"Distance from hole (ft)\")\nplot!(post_lines, color = :black, label = \"Logistic regression\")\nplot!(geom_lines, color = 1, label = \"Geometry-based model\")\n\nWe see that the geometry based model fits the data much better than the Logistic regression.\nNow let’s perform a posterior predictive check:\nangle_post = StatsBase.sample(chn2[:sigma].value, 500)\nangle_of_shot = rand.(Normal.(0, angle_post), 1)  # radians\nangle_of_shot = getindex.(angle_of_shot) # extract array\n\ndistance = 20 # feet\n\nend_positions = [\n    distance * cos.(angle_of_shot),\n    distance * sin.(angle_of_shot)\n]\n\n# visualize\nplot(\n  [[0, i] for i in end_positions[1]],\n  [[0, i] for i in end_positions[2]],\n  xlim = (-21, 21),\n  ylim = (-21, 21),\n  labels = false,\n  legend = :topleft,\n  color = :black,\n  alpha = 0.1,\n  title = \"Posterior distribution of putts from 20ft away\"\n  )\nscatter!(end_positions[1], end_positions[2], color = :black, labels = false)\nscatter!((0,0), color = :green, label = \"start\", markersize = 6)\nscatter!((20, 0), color = :red, label = \"goal\", markersize = 6)\n\nNew Golf Data\nThen came new data. Dr. Gelman received new data of golf putting and compared the fit of the original geometry based model with the old and new data. Here is the comparison below:\nrespnew = HTTP.get(\"https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/golf/golf_data_new.txt\");\ndatanew = CSV.read(IOBuffer(respnew.body), normalizenames = true, header = 3)\n\nxnew, nnew, ynew = (datanew[:,1], datanew[:,2], datanew[:,3])\npnew = ynew ./ nnew\nxrngnew = 1:1:80\n\n# plot the old model fit with new data\nthreshold_angle2 = [asin((R - r) / x) for x = xrngnew]\ngeom_lines2 = prob_angle(threshold_angle2, post_sigma)\n\nscatter(\n  x, pj,\n  label = \"Old data\",\n  ylab = \"Probability of Success\",\n  xlab = \"Distance from hole (ft)\",\n  color = 1)\nscatter!(xnew, pnew, color = 2, label = \"New data\")\nplot!(geom_lines2, label = \"\", color = 1)\n\nWe see that the new data have many more observations with longer distance putts than in the original data. The probability of success for these longer distance putts do not agree with the original geometry based model.\nUpdated Geometry\nSo Dr. Gelman improves the model by taking distance into account.\n\nTo get the ball in the hole, the angle isn’t the only thing you need to control; you also need to hit the ball just hard enough.\n…the probability a shot goes in becomes, \\[\n\\left(2\\Phi\\left(\\frac{\\sin^{-1}((R-r)/x)}{\\sigma_{\\rm angle}}\\right) - 1\\right)\\left(\\Phi\\left(\\frac{2}{(x+1)\\,\\sigma_{\\rm distance}}\\right) - \\Phi\\left(\\frac{-1}{(x+1)\\,\\sigma_{\\rm distance}}\\right)\\right)\n\\]\nwhere we have renamed the parameter σ from our earlier model to σ_angle to distinguish it from the new σ_distance parameter.\n\nLet’s add a function for the distance calculation for the improved geometry.\nprob_distance(distance, tol, overshot, sigma) =\n  Phi((tol - overshot) ./ ((distance .+ overshot) * sigma)) -\n    Phi(-overshot ./ ((distance .+ overshot) * sigma));\nNow let’s create the model to sample:\n@model golf_angle_dist(x, y, n, J, r, R, overshot, distance_tolerance) = begin\n  # transformed data\n  threshold_angle = asin.((R - r) ./ x)\n\n  # parameters\n  sigma_angle ~ truncated(Normal(0, 1), 0, Inf)\n  sigma_distance ~ truncated(Normal(0, 1), 0, Inf)\n\n  # model\n  p_angle = prob_angle(threshold_angle, sigma_angle)\n  p_distance = prob_distance(x, distance_tolerance, overshot, sigma_distance)\n  p = p_angle .* p_distance\n\n  for i in 1:J\n    y[i] ~ Binomial(n[i], p[i])\n  end\nend\n\novershot = 1.\ndistance_tolerance = 3.\n\nchn3 = sample(\n  golf_angle_dist(xnew, ynew, nnew, length(xnew), r, R, overshot, distance_tolerance),\n  NUTS(),\n  MCMCThreads(),\n  6000,\n  4)\n\nprettystats(chn3)\nparameters\nmean\nstd\nnaive_se\nmcse\ness\nr_hat\nsigma_angle\n0.014\n0.002\n0.0\n0.0\n80.321\n1.317\nsigma_distance\n0.135\n0.01\n0.0\n0.001\n80.321\n1.36\nDr. Gelman’s post suggests something is unstable with this model estimation. The estimated parameters match his closely. I did notice that sampling this model did not always give me same results though. I ended up increasing the length of the chains to try to get around the inconsistent parameter estimation I was experiencing.\nNow let’s make some predictions and visualize the results:\n# calculate predictions\npost_siga = median(chn3[:sigma_angle].value)\npost_sigd = median(chn3[:sigma_distance].value)\n\np_angle = prob_angle(threshold_angle2, post_siga)\np_distance = prob_distance(xrngnew, distance_tolerance, overshot, post_sigd)\n\ngeom2_lines = p_angle .* p_distance\n\n# plot\nscatter(\n  xnew, pnew,\n  legend = false,\n  color = 2,\n  ylab = \"Probability of Success\",\n  xlab = \"Distance from hole (ft)\")\nplot!(geom2_lines, color = 2)\n\nNow this model fits the new data better than the original geometry based model but you can see an issue near the middle of the range of the x-axis (distance). Some of Gelman’s select comments and proposed fix:\n\nThere are problems with the fit in the middle of the range of x. We suspect this is a problem with the binomial error model, as it tries harder to fit points where the counts are higher. Look at how closely the fitted curve hugs the data at the very lowest values of x.\nTo fix this problem we took the data model, \\[ y_j \\sim \\mbox{binomial}(n_j, p_j)\\], and added an independent error term to each observation.\n…we first approximate the binomial data distribution by a normal and then add independent variance; thus: \\[ y_j/n_j \\sim \\mbox{normal}\\left(p_j, \\sqrt{p_j(1-p_j)/n_j + \\sigma_y^2}\\right) \\]\n\nA Dispersed Model\nNow let’s implement the changes referenced above within Turing:\n@model golf_angle_dist_resid(x, y, n, J, r, R, overshot, distance_tolerance, raw) = begin\n  # transformed data\n  threshold_angle = asin.((R - r) ./ x)\n\n  # parameters\n  sigma_angle ~ truncated(Normal(0, 1), 0, Inf)\n  sigma_distance ~ truncated(Normal(0, 1), 0, Inf)\n  sigma_y ~ truncated(Normal(0, 1), 0, Inf)\n\n  # model\n  p_angle = prob_angle(threshold_angle, sigma_angle)\n  p_distance = prob_distance(x, distance_tolerance, overshot, sigma_distance)\n  p = p_angle .* p_distance\n\n  for i in 1:J\n    raw[i] ~ Normal(p[i], sqrt(p[i] * (1-p[i]) / n[i] + sigma_y^2))\n  end\nend\n\nchn4 = sample(\n  golf_angle_dist_resid(xnew, ynew, nnew, length(xnew), r, R, overshot, distance_tolerance, ynew ./ nnew),\n  NUTS(),\n  MCMCThreads(),\n  4000,\n  4)\n\n# adding the conversion to degrees\nchns = hcat(chn4, Chains(chn4[:sigma_angle].value * 180 / π, [\"sigma_degrees\"]))\nprettystats(chns)\nparameters\nmean\nstd\nnaive_se\nmcse\ness\nr_hat\nsigma_angle\n0.018\n0.0\n0.0\n0.0\n5026.215\n1.001\nsigma_distance\n0.08\n0.001\n0.0\n0.0\n4901.694\n1.0\nsigma_y\n0.003\n0.001\n0.0\n0.0\n6563.99\n1.0\nsigma_degrees\n1.02\n0.006\n0.0\n0.0\n5026.215\n1.001\nCalculate predictions and visualize:\npost_siga = median(chn4[:sigma_angle].value)\npost_sigd = median(chn4[:sigma_distance].value)\n\np_angle2 = prob_angle(threshold_angle2, post_siga)\np_distance2 = prob_distance(xrngnew, distance_tolerance, overshot, post_sigd)\n\ngeom_lines2 = p_angle2 .* p_distance2\n\n# plot\nscatter(\n  xnew, pnew,\n  legend = false,\n  color = 2,\n  ylab = \"Probability of Success\",\n  xlab = \"Distance from hole (ft)\")\nplot!(geom_lines2, color = 2)\n\nWe can see that this adjusted first principles based model is fitting the data much better now! To add to that, it also sampled faster and more consistently during my testing. This case study really shows off the power of a bayesian approach. The modeler has the ability to expand a model using domain knowledge and craft a model that makes sense and aligns with the data generating process.\nConclusion\nIf you made it this far, thanks for checking out this post! I personally appreciate all of the great scientists, applied statisticians, etc. that have created and shared the other posts I referenced as well as the team developing the Turing PPL within Julia. It’s really exciting to see a PPL written entirely in one language. In my opinion, it shows the strengths of the Julia language and is an example of its promise to solve the two-language problem.\nReferences\nStan: a state-of-the-art platform for statistical modeling and high-performance statistical computation\nModel building and expansion for golf putting - Gelman\nModel building and expansion for golf putting - Carroll\nBayesian golf puttings, NUTS, and optimizing your sampling function with TensorFlow Probability\nTuring.jl: A library for robust, efficient, general-purpose probabilistic programming\nAnnouncing composable multi-threaded parallelism in Julia\nODSC East 2016 | Stefan Karpinski - “Solving the Two Language Problem”\n\n\n\n",
    "preview": "posts/2019-11-02-golf-turing//model-building-of-golf-putting-with-turingjl_files/figure-html5/featured.png",
    "last_modified": "2022-03-18T22:00:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-26-market-dem-with-iv/",
    "title": "Market Demand with Instrumental Variables",
    "description": "Using brms to reproduce IV analysis",
    "author": [
      {
        "name": "J. Duncan",
        "url": "https://jduncstats.com/post/2019-03-26_market-dem-with-iv/"
      }
    ],
    "date": "2019-03-26",
    "categories": [],
    "contents": "\nOverview\nSkipper Seabold, a well known contributor to the PyData community, recently gave a talk titled “What’s the Science in Data Science?”. His talk presents several methods commonly used in econometrics that could benefit the field of data science if more widely adopted. Some of the methods were:\nInstrumental Variables\nMatching\nDifference-in-Differences\nThese modeling techniques are popular for discovering causal relationships in observational studies. When an RCT (randomized control trial) is unreasonable then these techniques can sometimes give us an alternative approach.\nI’ll be exploring one of these methods called Instrumental Variables (or IV). An example use case from Skipper’s talk was a study on the Fulton Fish Market in NYC.\n\n\n\nThe referenced study estimates the demand curve for fish at the market. Finding the demand curve is unfortunately not as simple as regressing quantity on price. The relationship between price and the quantity of fish sold is not exclusive to demand but includes supply effects at the market. We’ll be using IV to account for supply effects to isolate the demand effects.\nIn this blog post I’ll be reproducing a portion of this analysis using R, packages of the tidyverse, and the brms package in R.\nData\nI discovered data related to this study at the following website: http://people.brandeis.edu/~kgraddy/data.html\nBringing this data into R is very simple. The linked dataset appears to be the cleaned and transformed data used within the paper. Let’s read it into our R environment and take a look:\n\n\nlibrary(tidyverse)\n\nfulton <- read_tsv(\"http://people.brandeis.edu/~kgraddy/datasets/fish.out\")\nfulton\n\n\n# A tibble: 111 x 16\n    day1  day2  day3  day4   date stormy mixed   price   qty rainy\n   <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>   <dbl> <dbl> <dbl>\n 1     1     0     0     0 911202      1     0 -0.431   8.99     1\n 2     0     1     0     0 911203      1     0  0       7.71     0\n 3     0     0     1     0 911204      0     1  0.0723  8.35     1\n 4     0     0     0     1 911205      1     0  0.247   8.66     0\n 5     0     0     0     0 911206      1     0  0.664   7.84     0\n 6     1     0     0     0 911209      0     0 -0.207   9.30     0\n 7     0     1     0     0 911210      0     1 -0.116   8.92     0\n 8     0     0     1     0 911211      0     0 -0.260   9.11     1\n 9     0     0     0     1 911212      0     1 -0.117   8.31     0\n10     0     0     0     0 911213      0     0 -0.342   9.21     0\n# … with 101 more rows, and 6 more variables: cold <dbl>,\n#   windspd <dbl>, windspd2 <dbl>, pricelevel <dbl>, totr <dbl>,\n#   tots <dbl>\n\nHere is a quick description of the variables we will be using for this work:\nVariable\nUnits\nDescription\nqty\nlog(pounds)\nThe total amount of fish sold on a day\nprice\nlog($/lb)\nAverage price for the day\nday1-day4\ndummy var\nMonday-Thurs\ncold\ndummy var\nWeather on shore\nrainy\ndummy var\nRain on shore\nstormy\ndummy var\nWind and waves off shore (a 3-day moving average)\nThe paper informs the reader that transactions recorded are of a particular fish species called Whiting. We can get a feel for the total amount of Whiting being sold by reproducing Figure 2 from the paper.\n\n\n# fixing the date column \nfulton %>% \n  mutate(\n    date = as.character(date),\n    date = parse_date(date, format = \"%y%m%d\")\n  ) %>% \n  ggplot(aes(x = date, y = exp(qty))) +\n    geom_col() +\n    labs(\n      title = \"Figure 2\",\n      subtitle = \"Daily Volumes of Whiting\",\n      x = \"Date (December 2, 1991-May 8, 1992)\",\n      y = \"Quantity (pounds)\"\n      )\n\n\n\n\nReproducing this figure gave me confidence that I had the correct data to reproduce the analysis.\nDemand Curve\nThe naive approach to finding the relationship between price and demand (the demand curve) would be to regress quantity on price:\n\n\nggplot(fulton, aes(x = price, y = qty)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\nBut would this be the demand curve? No, it actually wouldn’t be. According to intro economics, each one of the points in the plot above is the result of the intersection of both a supply and demand curve. Something like this figure:\n\n\n\nSo how does the author go about estimating a more accurate representation of the demand curve? How do you isolate the demand effects from the supply? Well, it’s in the title of this post: Instrumental Variables.\nSkipper’s presentation explained IV as:\n\nWe can replace X with “instruments” that are correlated with X but not caused by Y or that affect Y but only through X.\n\nRegressions\nThe paper includes a table of estimated coefficients shown here:\n\n\n\nI’ll be reproducing this table but with Bayesian estimation. A nice benefit of using Bayesian estimation is that our estimated model includes distributions for each of our parameters. We’ll be visualizing these distributions for comparing results to the table above.\nOLS (Ordinary Least Squares) Reproduced\nFirst, let’s perform the classic linear regression. We can use the brm function in the brms package as a drop in replacement for R’s linear model function lm. However, the estimation process of lm and brm are quite different. The lm function is using OLS and the brms package is performing Bayesian estimation using a form of Markov Chain Monte Carlo (MCMC).\nStarting with column 1 of table 2 we estimate the coefficients with only qty and price:\n\n\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggridges)\n\n# column 1 in table 2\nfit1 <- brm(qty ~ price, data = fulton, refresh = 0)\n\nfit1 %>% \n  posterior_samples() %>% \n  ggplot(aes(x = b_price)) +\n    geom_density_line()\n\n\n\n\nThe brms package provides an abstraction layer to the Stan probabilistic programming language. So if you’re curious what the weakly informative priors that I breezed over actually are, take a look at the generated code with the stancode function. Here is an excerpt of the stan code generated for the model above:\n\nmodel {\n  vector[N] mu = temp_Intercept + Xc * b;\n  // priors including all constants\n  target += student_t_lpdf(temp_Intercept | 3, 9, 10);\n  target += student_t_lpdf(sigma | 3, 0, 10)\n    - 1 * student_t_lccdf(0 | 3, 0, 10);\n  // likelihood including all constants\n  if (!prior_only) {\n    target += normal_lpdf(Y | mu, sigma);\n  }\n}\n\nMoving to column 2 of table 2 we estimate the model including the dummy day variables, cold, and rainy variables:\n\n\n# column 2 in table 2\nfit1_full <- brm(\n  qty ~ price + day1 + day2 + day3 + day4 + cold + rainy,\n  data = fulton,\n  refresh = 0\n)\n\n# a plot of parameter estimates\nfit1_full %>% \n  posterior_samples() %>% \n  gather(b_price:b_rainy, key = \"coef\", value = \"est\") %>% \n  ggplot(aes(x = est, y = coef)) +\n    geom_density_ridges()\n\n\n\n\nWe can see that the estimated coefficient for price in this context is the same for both the simple linear regression and the regression accounting for weekday and onshore weather.\nIV Reproduced\nBefore performing the estimation I wanted to add a quote from the paper. I thought Kathryn Graddy explained the IV estimation well:\n\nThat is, first a regression is run with log price as the dependent variable and the storminess of the weather as the explanatory variable. This regression seeks to measure the variation in price that is attributable to stormy weather. The coefficients from this regression are then used to predict log price on each day, and these predicted values for price are inserted back into the regression.\n\nKathryn mentions two steps here:\nA regression with log price and storminess\nUsing coefficients from step 1, predict log price and place the predicted values back into the second regression.\nA simple diagram of this two step process is generated below:\n\n\nlibrary(ggdag)\n\ndagify(\n  qty ~ price, \n  price ~ stormy\n  ) %>% \n  ggdag(seed = 12) +\n  theme(\n    panel.background = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n    )\n\n\n\n\nNow let’s perform the estimation. This is possible with the brms package and its ability to specify multivariate response models. We’ll piece it together with two separate formulas defined in the bf function calls below.\nThe first estimation will be for column 3 of table 2:\n\n\n# measure variation in price attributable to stormy weather\nfit2a <- bf(price ~ stormy)\n# estimate demand\nfit2b <- bf(qty ~ price)\n\n# column 3 in table 2\nfit2 <- brm(fit2a + fit2b, data = fulton, refresh = 0)\n\nfit2 %>% \n  posterior_samples() %>% \n  ggplot(aes(x = b_qty_price)) +\n    geom_density_line()\n\n\n\n\nAnd lastly we’ll estimate the IV with the remaining variables (column 4):\n\n\n# visual representation of the estimation\ndagify(\n  qty ~ price + day1 + day2 + day3 + day4 + cold + rainy, \n  price ~ stormy\n  ) %>% \n  ggdag(seed = 9) +\n  theme(\n    panel.background = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank()\n    )\n\n\n\n\n\n\n# add additional demand specific variables\nfit2b_full <- bf(qty ~ price + day1 + day2 + day3 + day4 + cold + rainy)\n\n# column 4 in table 2\nfit2_full <- brm(fit2a + fit2b_full, data = fulton, refresh = 0)\n\nfit2_full %>% \n  posterior_samples() %>% \n  gather(b_qty_price:b_qty_rainy, key = \"coef\", value = \"est\") %>% \n  ggplot(aes(x = est, y = coef)) +\n    geom_density_ridges()\n\n\n\n\nOne thing that I think is particularly interesting about the distributions above is the uncertainty of the price coefficient. We can clearly see that the parameter’s distribution with IV is much wider than our classic linear regression. Here we are seeing the uncertainty of our first model (price ~ storminess) propagating to our second model.\nVisualizing the Demand Curve\nNow we can plot the demand curve with isolated demand effects. I’ll use the first IV estimation with only qty, price, and stormy variables:\n\n\nfulton %>% \n  add_predicted_draws(fit2) %>% \n  filter(.category == \"qty\") %>% \n  ggplot(aes(x = price, y = qty)) +\n  stat_lineribbon(\n    aes(y = .prediction), \n    .width = c(.99, .95, .8, .5), \n    color = \"#08519C\"\n    ) +\n  geom_point(data = fulton, size = 2) +\n  scale_fill_brewer()\n\n\n\n\nThe prediction is downward trending as the original curve except with a steeper slope (-0.54 vs -1.06).\nElasticities\nThe paper breaks down the interpretation of the estimated coefficients in terms of elasticities. Given that we have taken the log of both our quantity and price the coefficients can be interpreted in a clever way. Let’s take a look at why this is the case:\nIf we take the log of both \\(y\\) and \\(x\\) of our linear model:\n\\[\n\\text{log}(y) = \\beta_0 + \\beta_1\\text{log}(x) + \\epsilon\n\\] Now solve for \\(y\\) to find the marginal effects:\n\\[\ny = e^{\\beta_o + \\beta_1\\text{log}(x) + \\epsilon}\n\\] Then differentiate with respect to \\(x\\):\n\\[\n\\frac{dy}{dx} = \\frac{\\beta_1}{x}e^{\\beta_o + \\beta_1\\text{log}(x) + \\epsilon} = \\beta_1 \\frac{y}{x}\n\\]\nIf you then solve for \\(\\beta_1\\) you find:\n\\[\n\\beta_1 = \\frac{dy}{dx} \\frac{x}{y}\n\\] So here \\(\\beta_1\\) is an elasticity. For a \\(\\%\\) increase in \\(x\\) there is a \\(\\beta_1 \\%\\) increase in \\(y\\).\nElasticities are commonly summarized in a table like this:\nElasticity\nValue\nDescription\nElastic\n| E | > 1\n% change in Q > % change in P\nUnitary Elastic\n| E | = 1\n% change in Q = %change in P\nInelastic\n| E | < 1\n% change in Q < % change in P\nGiven the descriptions of elasticity above. We would have two different interpretations of how demand responds to price with the non-IV and the IV estimation. With the non-IV estimation our elasticity coefficient is -0.54 [-0.89, -0.19]. With the IV estimation our elasticity is -1.06 [-2.18, -0.15]. So we would mistakenly interpret the demand elasticity as being inelastic when it actually appears to be unit elastic.\nSome interesting interpretations of this unit elasticity from the paper include:\n\nFirst, it is consistent with pricing power on the part of the fish dealers. A price-setting firm will raise price to the point where the percentage change in the quantity demanded is at least as large as the percentage change in price; otherwise, it would make sense to raise the price even more\nSecond, when demand has a unitary elasticity, it means that the percentage change in quantity would always equal the percentage change in price, and the weather would therefore not have much effect on a seller’s revenue, keeping fishermen’s incomes relatively constant.\nThird, unit elasticities could also result from budget constraints on the part of some buyers.\n\nWrapping Up\nThis was definitely a fun topic to start exploring. I plan on taking a look at difference-in-differences in the near future as well.\nReferences\nWhat’s the Science in Data Science?\nGraddy, Kathryn. 2006. “Markets: The Fulton Fish Market.” Journal of Economic Perspectives, 20 (2): 207-220.\nCreate supply and demand economics curves with ggplot2\nMichael Betancourt: “A Conceptual Introduction to Hamiltonian Monte Carlo”, 2017\n\n\n\n",
    "preview": "posts/2019-03-26-market-dem-with-iv/market-demand-with-instrumental-variables_files/figure-html5/fulton.jpg",
    "last_modified": "2022-03-18T22:00:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-16-kde-scratch/",
    "title": "Kernel Density Estimation from Scratch",
    "description": "Implementing KDE with Julia",
    "author": [
      {
        "name": "J. Duncan",
        "url": "https://jduncstats.com/post/2019-03-16_kde-scratch/"
      }
    ],
    "date": "2019-03-16",
    "categories": [],
    "contents": "\nMotivation\nWhy am I writing about KDE’s? At the recommendation of Will Kurt’s probability blog I’ve been\nreading a great book on data analysis by Philipp K. Janert:\n\n\n\nThis book has a great intro to KDEs that explain the motivation. My\nown abbreviated version are that KDEs provide a useful technique to\nvisualize a variables distribution. Visualizing your data is an\nimportant step to take early in the data analysis stage. In fact, there\nare many metrics that we commonly use to understand a variable that have\nan implicit assumption that your data are unimodal (having\na single peak). If your data doesn’t have this structure then you may be\nmislead by measures of central tendency (mean/median/mode), outliers, or\nother statistical methods (linear regression, t-tests, etc.).\nThis post’s structure follows closely with how I commonly learn\ntopics. I start at a high level, using a pre-canned solution for the\nalgorithm, and then work backward to find out what’s going on\nunderneath.\nI use a small example I discovered on the Wikipedia\npage for KDEs. It uses a handful of data points for a single\nvariable:\nSample\nValue\n1\n-2.1\n2\n-1.3\n3\n-0.4\n4\n1.9\n5\n5.1\n6\n6.2\nLet’s start with a basic dot plot of these points. I’ll\nbe using the Julia Programming Language for these\nexamples.\nusing StatsPlots\n\n# initialize an array of the samples\nx = [-2.1; -1.3; -0.4; 1.9; 5.1; 6.2];\n# plot it out\nscatter(x, zeros(length(x)), legend = false)\n\nKDE with KernelDensity.jl\nNow applying the quick and easy solution: a package. This\npackage has a function named kde that takes a one\ndimensional array (or vector), a bandwidth argument, and a chosen kernel\n(we’ll use the default). So let’s see it:\nimport KernelDensity\n\nKernelDensity.kde(x, bandwidth = sqrt(2.25)) |>\n  x -> plot!(x, legend = false)\n\nThere we go, we’ve applied KDE to these data points and we can now\nsee the bimodal nature of this data. If all we wanted to do\nwas visualize the distribution then we’re done. I’d like to dig a bit\ndeeper though.\nKDE with Distributions.jl\nWhat is the kernel part of this about? What was the\ndefault kernel we used in the previous section? The kde\nfunction from the package used a default kernel associated with the\nNormal distribution. But to understand what this all means we need to\ntake a look at the definition of Kernel Density Estimation:\n\\[\nD_h(x; {x_i}) = \\sum_{i=1}^n \\frac{1}{nh} K\\left(\\frac{x -\nx_i}{h}\\right)\n\\]\nBreaking down this formula a bit: The kernel is the function shown\nabove as $K$ and Janert describes it like so:\n\nTo form a KDE, we place a kernel —that is, a smooth,\nstrongly peaked function—at the position of each data point. We then add\nup the contributions from all kernels to obtain a smooth curve, which we\ncan evaluate at any point along the x axis\n\nWe are effectively calculating weighted distances from our data\npoints to points along the x axis. There is a great interactive\nintroduction to kernel density estimation here. I highly recommend\nit because you can play with bandwidth, select different kernel methods,\nand check out the resulting effects.\nAs I mentioned before, the default kernel for this\npackage is the Normal (or Gaussian) probability density function\n(pdf):\n\\[\nK(x) = \\frac{1}{\\sqrt{2\\pi}}\\text{exp}\\left(-\\frac{1}{2}x^2\\right)\n\\]\nSince we are calculating pdfs I’ll use the Distributions.jl\npackage to create each distribution, calculate the densities, and sum\nthe results.\nusing Distributions\n\ndists = Normal.(x, sqrt(2.25))\n6-element Array{Distributions.Normal{Float64},1}:\n Distributions.Normal{Float64}(μ=-2.1, σ=1.5)\n Distributions.Normal{Float64}(μ=-1.3, σ=1.5)\n Distributions.Normal{Float64}(μ=-0.4, σ=1.5)\n Distributions.Normal{Float64}(μ=1.9, σ=1.5)\n Distributions.Normal{Float64}(μ=5.1, σ=1.5)\n Distributions.Normal{Float64}(μ=6.2, σ=1.5)\nHere we see a neat feature of the Julia language. Any Julia function\ncan be vectorized (or broadcasted) by the application of the\n. (or “dot”) operator. See this\nblog post if you want to learn more about it. Above we applied the\nNormal method element-wise creating an array of Normal\ndistributions. The mean of our individual distributions being our data\npoints and a variance of 2.25 (aka our chosen bandwidth).\nLet’s plot each of these distributions:\nplot(dists, legend = false)\n\nSumming up their probability densities across all of\nx.\n# create an iterator\nx_d = range(-7, 11, length = 100)\n# find the kde with a gaussian kernel\ndens = sum(pdf.(eachdist, x_d) for eachdist in dists)\n\nplot!(x_d, dens)\n\nThe resulting shape of the KDE is identical to the one we first\ncalculated. We could stop here except this is really just a special case\nwhere we are using the gaussian kernel. Let’s extrapolate a bit so we\ncould use different kernels.\nKernel Density from Scratch\nTo apply a new kernel method we can just write the KDE code from\nscratch. Below I’ve defined the KDE function as D and the\nkernel argument as K to mimic the math above.\n# define some kernels:\n# gaussian kernel\nkgauss(x) = 1/sqrt(2π) * exp(-1/2 * x^2)\n# boxcar\nkbox(x) = abs(x) <= 1 ? 1/2 : 0\n# triangular\nktri(x) = abs(x) <= 1 ? 1 - abs(x) : 0\n\n# define the KDE function\nD(x, h, xi, K) =\n  1/(length(xi) * h) * sum(K.((x .- xi) / h))\n\n# evaluate KDE along the x-axis using comprehensions\ndens = [D(xstep, sqrt(2.25), x, K) for xstep in x_d, K in (kgauss, kbox, ktri)]\n\n# visualize the kernels\nplot(x_d, dens, label = [\"Gaussian\", \"Box\", \"Triangular\"])\n\nIn my example above I used some shorthand Julia syntax. The\n?: syntax is called a ternary operator and makes a\nconditional if-else statement more compact. I also used\nJulia’s Assignment form for my function definitions above\nbecause it looks a lot more like the math involved. You could have\neasily defined each of these functions to look more like so:\nfunction foo(x)\n  if ...\n    ...\n  else\n    ...\n  end\n  return ...\nend\nWhat’s next?\nA short post on cumulative distribution functions (cdf) using Julia\nwill likely follow this one. Janert introduces both kdes and cdfs in his\nchapter A Single Variable: Shape and Distribution and\nthey complement each other really well. Thanks for reading!\n\n\n\n",
    "preview": "posts/2019-03-16-kde-scratch//kernel-density-estimation-from-scratch_files/figure-html5/featured.png",
    "last_modified": "2022-03-18T22:34:14-05:00",
    "input_file": {}
  }
]
