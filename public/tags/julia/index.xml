<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>julia on Josh Duncan</title>
    <link>/tags/julia/</link>
    <description>Recent content in julia on Josh Duncan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Nov 2019 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/tags/julia/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Model building of golf putting with Turing.jl</title>
      <link>/post/2019-11-02_golf-turing/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 -0500</pubDate>
      
      <guid>/post/2019-11-02_golf-turing/</guid>
      <description>First Thing (Disclaimer) This blog post is based on a blog post written by the popular bayesian statistician Andrew Gelman.
Dr. Gelman&amp;rsquo;s post was written in the Stan probabilistic programming language (or PPL). Since this post, there have been a couple other blog posts translating the code to other PPLs. They are excellent and I definitely recommend checking them out. Here&amp;rsquo;s the list of links and my opinion on some of the benefits of reading each:</description>
    </item>
    
    <item>
      <title>Kernel Density Estimation from Scratch</title>
      <link>/post/2019-03-16_kde-scratch/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 -0500</pubDate>
      
      <guid>/post/2019-03-16_kde-scratch/</guid>
      <description>Motivation Why am I writing about KDE&amp;rsquo;s? At the recommendation of Will Kurt&amp;rsquo;s probability blog I&amp;rsquo;ve been reading a great book on data analysis by Philipp K. Janert:
This book has a great intro to KDEs that explain the motivation. My own abbreviated version are that KDEs provide a useful technique to visualize a variables distribution. Visualizing your data is an important step to take early in the data analysis stage.</description>
    </item>
    
  </channel>
</rss>