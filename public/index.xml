<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Josh Duncan on Josh Duncan</title>
    <link>/</link>
    <description>Recent content in Josh Duncan on Josh Duncan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Sep 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Interests</title>
      <link>/bio/skills/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 -0500</pubDate>
      
      <guid>/bio/skills/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Work Experience</title>
      <link>/bio/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/bio/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 -0500</pubDate>
      
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;em&gt;Slides&lt;/em&gt; feature and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Market Demand with Instrumental Variables</title>
      <link>/post/2019-03-26_market-dem-with-iv/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-03-26_market-dem-with-iv/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Skipper Seabold, a well known contributor to the PyData community, recently gave a &lt;a href=&#34;https://youtu.be/kTo16ieMCi8&#34;&gt;talk&lt;/a&gt; titled “What’s the Science in Data Science?”. His talk presents several methods commonly used in econometrics that could benefit the field of data science if more widely adopted. Some of the methods were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instrumental Variables&lt;/li&gt;
&lt;li&gt;Matching&lt;/li&gt;
&lt;li&gt;Difference-in-Differences&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From what I gather, these modeling techniques are popular for discovering causal relationships in observational studies. When an RCT (randomized control trial) is unreasonable then these techniques give us an alternative approach.&lt;/p&gt;
&lt;p&gt;I’ll be exploring one of these methods called &lt;em&gt;Instrumental Variables&lt;/em&gt; (or IV). An example use case from Skipper’s talk was a &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.20.2.207&#34;&gt;study&lt;/a&gt; on the &lt;strong&gt;Fulton Fish Market&lt;/strong&gt; in NYC.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26-market-price-with-instrumental-variables_files/fulton.jpg&#34; alt=&#34;drawing&#34; width=&#34;50%&#34;/&gt;&lt;/p&gt;
&lt;p&gt;The referenced study estimates the demand curve for fish at the market. Finding the demand curve is unfortunately not as simple as regressing quantity on price. The relationship between price and the quantity of fish sold is not exclusive to demand but includes supply effects at the market. We’ll be using IV to account for supply effects to isolate the demand effects.&lt;/p&gt;
&lt;p&gt;In this blog post I’ll be reproducing a portion of this analysis using R, packages of the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;, and the &lt;a href=&#34;https://github.com/paul-buerkner/brms&#34;&gt;brms&lt;/a&gt; package in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;I discovered data related to this study at the following website:
&lt;a href=&#34;http://people.brandeis.edu/~kgraddy/data.html&#34; class=&#34;uri&#34;&gt;http://people.brandeis.edu/~kgraddy/data.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bringing this data into R is very simple. The linked dataset appears to be the cleaned and transformed data used within the paper. Let’s read it into our R environment and take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

fulton &amp;lt;- read_tsv(&amp;quot;http://people.brandeis.edu/~kgraddy/datasets/fish.out&amp;quot;)
fulton&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 111 x 16
##     day1  day2  day3  day4   date stormy mixed   price   qty rainy  cold
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1     1     0     0     0 911202      1     0 -0.431   8.99     1     0
##  2     0     1     0     0 911203      1     0  0       7.71     0     0
##  3     0     0     1     0 911204      0     1  0.0723  8.35     1     1
##  4     0     0     0     1 911205      1     0  0.247   8.66     0     1
##  5     0     0     0     0 911206      1     0  0.664   7.84     0     1
##  6     1     0     0     0 911209      0     0 -0.207   9.30     0     0
##  7     0     1     0     0 911210      0     1 -0.116   8.92     0     0
##  8     0     0     1     0 911211      0     0 -0.260   9.11     1     0
##  9     0     0     0     1 911212      0     1 -0.117   8.31     0     0
## 10     0     0     0     0 911213      0     0 -0.342   9.21     0     0
## # … with 101 more rows, and 5 more variables: windspd &amp;lt;dbl&amp;gt;,
## #   windspd2 &amp;lt;dbl&amp;gt;, pricelevel &amp;lt;dbl&amp;gt;, totr &amp;lt;dbl&amp;gt;, tots &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a quick description of the variables we will be using for this work:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;Units&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;qty&lt;/td&gt;
&lt;td&gt;log(pounds)&lt;/td&gt;
&lt;td&gt;The total amount of fish sold on a day&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;price&lt;/td&gt;
&lt;td&gt;log($/lb)&lt;/td&gt;
&lt;td&gt;Average price for the day&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;day1-day4&lt;/td&gt;
&lt;td&gt;dummy var&lt;/td&gt;
&lt;td&gt;Monday-Thurs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;cold&lt;/td&gt;
&lt;td&gt;dummy var&lt;/td&gt;
&lt;td&gt;Weather on shore&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;rainy&lt;/td&gt;
&lt;td&gt;dummy var&lt;/td&gt;
&lt;td&gt;Rain on shore&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;stormy&lt;/td&gt;
&lt;td&gt;dummy var&lt;/td&gt;
&lt;td&gt;Wind and waves off shore (a 3-day moving average)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The paper informs the reader that transactions recorded are of a particular fish species called Whiting. We can get a feel for the total amount of Whiting being sold by reproducing Figure &lt;code&gt;2&lt;/code&gt; from the paper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fixing the date column 
fulton %&amp;gt;% 
  mutate(
    date = as.character(date),
    date = parse_date(date, format = &amp;quot;%y%m%d&amp;quot;)
  ) %&amp;gt;% 
  ggplot(aes(x = date, y = exp(qty))) +
    geom_col() +
    labs(
      title = &amp;quot;Figure 2&amp;quot;,
      subtitle = &amp;quot;Daily Volumes of Whiting&amp;quot;,
      x = &amp;quot;Date (December 2, 1991-May 8, 1992)&amp;quot;,
      y = &amp;quot;Quantity (pounds)&amp;quot;
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Reproducing this figure gave me confidence that I had the correct data to reproduce the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;demand-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Demand Curve&lt;/h2&gt;
&lt;p&gt;The naive approach to finding the relationship between price and demand (the demand curve) would be to regress quantity on price:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(fulton, aes(x = price, y = qty)) + 
  geom_point() + 
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-3-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But would this be the demand curve? No, it actually wouldn’t be. According to intro economics, each one of the points in the plot above is the result of the intersection of both a supply and demand curve. Something like this &lt;a href=&#34;https://www.andrewheiss.com/blog/2017/09/15/create-supply-and-demand-economics-curves-with-ggplot2/&#34;&gt;figure&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26-market-price-with-instrumental-variables_files/supply-demand-intersection-simple-1.png&#34; alt=&#34;drawing&#34; width=&#34;80%&#34;/&gt;&lt;/p&gt;
&lt;p&gt;So how does the author go about estimating a more accurate representation of the demand curve? How do you isolate the demand effects from the supply? Well, it’s in the title of this post: &lt;strong&gt;Instrumental Variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Skipper’s presentation explained IV as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can replace &lt;em&gt;X&lt;/em&gt; with &lt;strong&gt;“instruments”&lt;/strong&gt; that are &lt;strong&gt;correlated with X&lt;/strong&gt; but &lt;strong&gt;not caused by Y&lt;/strong&gt; or that &lt;strong&gt;affect Y&lt;/strong&gt; but &lt;em&gt;only&lt;/em&gt; &lt;strong&gt;through X&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;regressions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regressions&lt;/h2&gt;
&lt;p&gt;The paper includes a table of estimated coefficients shown here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26-market-price-with-instrumental-variables_files/table-2.png&#34; alt=&#34;drawing&#34; width=&#34;80%&#34;/&gt;&lt;/p&gt;
&lt;p&gt;I’ll be reproducing this table but with Bayesian estimation. A nice benefit of using Bayesian estimation is that our estimated model includes distributions for each of our parameters. We’ll be visualizing these distributions for comparing results to the table above.&lt;/p&gt;
&lt;div id=&#34;ols-ordinary-least-squares-reproduced&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;OLS (Ordinary Least Squares) Reproduced&lt;/h3&gt;
&lt;p&gt;First, let’s perform the classic linear regression. We can use the &lt;code&gt;brm&lt;/code&gt; function in the &lt;code&gt;brms&lt;/code&gt; package as a drop in replacement for R’s linear model function &lt;code&gt;lm&lt;/code&gt;. However, the estimation process of &lt;code&gt;lm&lt;/code&gt; and &lt;code&gt;brm&lt;/code&gt; are quite different. The &lt;code&gt;lm&lt;/code&gt; function is using OLS and the &lt;code&gt;brms&lt;/code&gt; package is performing Bayesian estimation using a &lt;a href=&#34;https://arxiv.org/abs/1701.02434&#34;&gt;form&lt;/a&gt; of Markov Chain Monte Carlo (MCMC).&lt;/p&gt;
&lt;p&gt;Starting with column &lt;code&gt;1&lt;/code&gt; of table &lt;code&gt;2&lt;/code&gt; we estimate the coefficients with only &lt;code&gt;qty&lt;/code&gt; and &lt;code&gt;price&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)
library(tidybayes)
library(ggridges)

# column 1 in table 2
fit1 &amp;lt;- brm(qty ~ price, data = fulton, refresh = 0)

fit1 %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  ggplot(aes(x = b_price)) +
    geom_density_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;brms&lt;/code&gt; package provides an abstraction layer to the &lt;code&gt;Stan&lt;/code&gt; probabilistic programming language. So if you’re curious what the “uninformative” priors that I breezed over actually are, take a look at the generated code with the &lt;code&gt;stancode&lt;/code&gt; function. Here is an excerpt of the stan code generated for the model above:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;model {
  vector[N] mu = temp_Intercept + Xc * b;
  // priors including all constants
  target += student_t_lpdf(temp_Intercept | 3, 9, 10);
  target += student_t_lpdf(sigma | 3, 0, 10)
    - 1 * student_t_lccdf(0 | 3, 0, 10);
  // likelihood including all constants
  if (!prior_only) {
    target += normal_lpdf(Y | mu, sigma);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Moving to column &lt;code&gt;2&lt;/code&gt; of table &lt;code&gt;2&lt;/code&gt; we estimate the model including the dummy &lt;code&gt;day&lt;/code&gt; variables, &lt;code&gt;cold&lt;/code&gt;, and &lt;code&gt;rainy&lt;/code&gt; variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# column 2 in table 2
fit1_full &amp;lt;- brm(
  qty ~ price + day1 + day2 + day3 + day4 + cold + rainy,
  data = fulton,
  refresh = 0
)

# a plot of parameter estimates
fit1_full %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  gather(b_price:b_rainy, key = &amp;quot;coef&amp;quot;, value = &amp;quot;est&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = est, y = coef)) +
    geom_density_ridges()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-6-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the estimated coefficient for price in this context is the same for both the simple linear regression and the regression accounting for weekday and onshore weather.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iv-reproduced&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;IV Reproduced&lt;/h3&gt;
&lt;p&gt;Before performing the estimation I wanted to add a quote from the paper. I thought Kathryn Graddy explained the IV estimation well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;That is, first a regression is run with log price as the dependent variable and the storminess of the weather as the explanatory variable. This regression seeks to measure the variation in price that is attributable to stormy weather. The coefficients from this regression are then used to predict log price on each day, and these predicted values for price are inserted back into the regression.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kathryn mentions two steps here:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A regression with log price and storminess&lt;/li&gt;
&lt;li&gt;Using coefficients from step &lt;code&gt;1&lt;/code&gt;, predict log price and place the predicted values back into the second regression.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A simple diagram of this two step process is generated below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggdag)

dagify(
  qty ~ price, 
  price ~ stormy
  ) %&amp;gt;% 
  ggdag(seed = 12) +
  theme(panel.background = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-7-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s perform the estimation. This is possible with the &lt;code&gt;brms&lt;/code&gt; package and its ability to specify multivariate response models. We’ll piece it together with two separate formulas defined in the &lt;code&gt;bf&lt;/code&gt; function calls below.&lt;/p&gt;
&lt;p&gt;The first estimation will be for column &lt;code&gt;3&lt;/code&gt; of table &lt;code&gt;2&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# measure variation in price attributable to stormy weather
fit2a &amp;lt;- bf(price ~ stormy)
# estimate demand
fit2b &amp;lt;- bf(qty ~ price)

# column 3 in table 2
fit2 &amp;lt;- brm(fit2a + fit2b, data = fulton, refresh = 0)

fit2 %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  ggplot(aes(x = b_qty_price)) +
    geom_density_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-8-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And lastly we’ll estimate the IV with the remaining variables (column &lt;code&gt;4&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visual representation of the estimation
dagify(
  qty ~ price + day1 + day2 + day3 + day4 + cold + rainy, 
  price ~ stormy
  ) %&amp;gt;% 
  ggdag(seed = 9) +
  theme(panel.background = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add additional demand specific variables
fit2b_full &amp;lt;- bf(qty ~ price + day1 + day2 + day3 + day4 + cold + rainy)

# column 4 in table 2
fit2_full &amp;lt;- brm(fit2a + fit2b_full, data = fulton, refresh = 0)

fit2_full %&amp;gt;% 
  posterior_samples() %&amp;gt;% 
  gather(b_qty_price:b_qty_rainy, key = &amp;quot;coef&amp;quot;, value = &amp;quot;est&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = est, y = coef)) +
    geom_density_ridges()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-10-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One thing that I think is particularly interesting about the distributions above is the uncertainty of the price coefficient. We can clearly see that the parameter’s distribution with IV is much wider than our classic linear regression. Here we are seeing the uncertainty of our first model (&lt;code&gt;price ~ storminess&lt;/code&gt;) propagating to our second model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-the-demand-curve&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualizing the Demand Curve&lt;/h3&gt;
&lt;p&gt;Now we can plot the demand curve with isolated demand effects. I’ll use the first IV estimation with only &lt;code&gt;qty&lt;/code&gt;, &lt;code&gt;price&lt;/code&gt;, and &lt;code&gt;stormy&lt;/code&gt; variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fulton %&amp;gt;% 
  add_predicted_draws(fit2) %&amp;gt;% 
  filter(.category == &amp;quot;qty&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = price, y = qty)) +
  stat_lineribbon(
    aes(y = .prediction), 
    .width = c(.99, .95, .8, .5), 
    color = &amp;quot;#08519C&amp;quot;
    ) +
  geom_point(data = fulton, size = 2) +
  scale_fill_brewer()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-26_market-dem-with-iv/index_files/figure-html/unnamed-chunk-11-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The prediction is downward trending as the original curve except with a steeper slope (&lt;code&gt;-0.54&lt;/code&gt; vs &lt;code&gt;-1.09&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;elasticities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Elasticities&lt;/h2&gt;
&lt;p&gt;The paper breaks down the interpretation of the estimated coefficients in terms of elasticities. Given that we have taken the &lt;code&gt;log&lt;/code&gt; of both our quantity and price the coefficients can be interpreted in a clever way. Let’s take a look at why this is the case:&lt;/p&gt;
&lt;p&gt;If we take the log of both &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; of our linear model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{log}(y) = \beta_0 + \beta_1\text{log}(x) + \epsilon
\]&lt;/span&gt;
Now solve for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; to find the marginal effects:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y = e^{\beta_o + \beta_1\text{log}(x) + \epsilon}
\]&lt;/span&gt;
Then differentiate with respect to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{dy}{dx} = \frac{\beta_1}{x}e^{\beta_o + \beta_1\text{log}(x) + \epsilon} = \beta_1 \frac{y}{x}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you then solve for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; you find:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta_1 = \frac{dy}{dx} \frac{x}{y}
\]&lt;/span&gt;
So here &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is an elasticity. For a &lt;span class=&#34;math inline&#34;&gt;\(\%\)&lt;/span&gt; increase in &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; there is a &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 \%\)&lt;/span&gt; increase in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Elasticities are commonly summarized in a table like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Elasticity&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Elastic&lt;/td&gt;
&lt;td&gt;| &lt;em&gt;E&lt;/em&gt; | &amp;gt; 1&lt;/td&gt;
&lt;td&gt;% change in &lt;em&gt;Q&lt;/em&gt; &amp;gt; % change in &lt;em&gt;P&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Unitary Elastic&lt;/td&gt;
&lt;td&gt;| &lt;em&gt;E&lt;/em&gt; | = 1&lt;/td&gt;
&lt;td&gt;% change in &lt;em&gt;Q&lt;/em&gt; = %change in &lt;em&gt;P&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Inelastic&lt;/td&gt;
&lt;td&gt;| &lt;em&gt;E&lt;/em&gt; | &amp;lt; 1&lt;/td&gt;
&lt;td&gt;% change in &lt;em&gt;Q&lt;/em&gt; &amp;lt; % change in &lt;em&gt;P&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Given the descriptions of elasticity above. We would have two different interpretations of how demand responds to price with the non-IV and the IV estimation. With the non-IV estimation our elasticity coefficient is &lt;code&gt;-0.54&lt;/code&gt; [-0.9, -0.18]. With the IV estimation our elasticity is &lt;code&gt;-1.09&lt;/code&gt; [-2.17, -0.15]. So we would mistakenly interpret the demand elasticity as being inelastic when it actually appears to be unit elastic.&lt;/p&gt;
&lt;p&gt;Some interesting interpretations of this unit elasticity from the paper include:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;First, it is consistent with pricing power on the part of the
fish dealers. A price-setting firm will raise price to the point where the percentage change in the quantity demanded is at least as large as the percentage change in price; otherwise, it would make sense to raise the price even more&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Second, when demand has a unitary elasticity, it means that the percentage change in quantity would always equal the percentage change in price, and the weather would therefore not have much effect on a seller’s revenue, keeping fishermen’s incomes relatively constant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Third, unit elasticities could also result from budget constraints on the part of some buyers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;This was definitely a fun topic to start exploring. I plan on taking a look at &lt;em&gt;difference-in-differences&lt;/em&gt; in the near future as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/kTo16ieMCi8&#34;&gt;What’s the Science in Data Science?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.20.2.207&#34;&gt;Graddy, Kathryn. 2006. “Markets: The Fulton Fish Market.” Journal of Economic Perspectives, 20 (2): 207-220.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.andrewheiss.com/blog/2017/09/15/create-supply-and-demand-economics-curves-with-ggplot2/&#34;&gt;Create supply and demand economics curves with ggplot2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.02434&#34;&gt;Michael Betancourt: “A Conceptual Introduction to Hamiltonian Monte Carlo”, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Kernel Density Estimation from Scratch</title>
      <link>/post/2019-03-16_kde-scratch/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 -0500</pubDate>
      
      <guid>/post/2019-03-16_kde-scratch/</guid>
      <description>

&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;Why am I writing about KDE&amp;rsquo;s? At the recommendation of Will Kurt&amp;rsquo;s &lt;a href=&#34;https://www.countbayesie.com/&#34; target=&#34;_blank&#34;&gt;probability blog&lt;/a&gt; I&amp;rsquo;ve been reading a great book on data analysis by Philipp K. Janert:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/janert.jpg&#34; alt=&#34;drawing&#34; width=&#34;50%&#34;/&gt;&lt;/p&gt;

&lt;p&gt;This book has a great intro to KDEs that explain the motivation. My own abbreviated version are that KDEs provide a useful technique to visualize a variables distribution. Visualizing your data is an important step to take early in the data analysis stage. In fact, there are many metrics that we commonly use to understand a variable that have an implicit assumption that your data are &lt;code&gt;unimodal&lt;/code&gt; (having a single peak). If your data doesn&amp;rsquo;t have this structure then you may be mislead by measures of central tendency (mean/median/mode), outliers, or other statistical methods (linear regression, t-tests, etc.).&lt;/p&gt;

&lt;p&gt;This post&amp;rsquo;s structure follows closely with how I commonly learn topics. I start at a high level, using a pre-canned solution for the algorithm, and then work backward to find out what&amp;rsquo;s going on underneath.&lt;/p&gt;

&lt;p&gt;I use a small example I discovered on the &lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34;&gt;Wikipedia page&lt;/a&gt; for KDEs. It uses a handful of data points for a single variable:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-2.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;-1.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;-0.4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;6.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let&amp;rsquo;s start with a basic &lt;code&gt;dot&lt;/code&gt; plot of these points. I&amp;rsquo;ll be using the &lt;code&gt;Julia Programming Language&lt;/code&gt; for these examples.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using StatsPlots

# initialize an array of the samples
x = [-2.1; -1.3; -0.4; 1.9; 5.1; 6.2];
# plot it out
scatter(x, zeros(length(x)), legend = false)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_1_1.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;kde-with-kerneldensity-jl&#34;&gt;KDE with KernelDensity.jl&lt;/h2&gt;

&lt;p&gt;Now applying the quick and easy solution: a &lt;a href=&#34;https://github.com/JuliaStats/KernelDensity.jl&#34; target=&#34;_blank&#34;&gt;package&lt;/a&gt;. This package has a function named &lt;code&gt;kde&lt;/code&gt; that takes a one dimensional array (or vector), a bandwidth argument, and a chosen kernel (we&amp;rsquo;ll use the default). So let&amp;rsquo;s see it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;import KernelDensity

KernelDensity.kde(x, bandwidth = sqrt(2.25)) |&amp;gt;
  x -&amp;gt; plot!(x, legend = false)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_2_1.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There we go, we&amp;rsquo;ve applied KDE to these data points and we can now see the &lt;code&gt;bimodal&lt;/code&gt; nature of this data. If all we wanted to do was visualize the distribution then we&amp;rsquo;re done. I&amp;rsquo;d like to dig a bit deeper though.&lt;/p&gt;

&lt;h2 id=&#34;kde-with-distributions-jl&#34;&gt;KDE with Distributions.jl&lt;/h2&gt;

&lt;p&gt;What is the &lt;code&gt;kernel&lt;/code&gt; part of this about? What was the default kernel we used in the previous section? The &lt;code&gt;kde&lt;/code&gt; function from the package used a default kernel associated with the Normal distribution. But to understand what this all means we need to take a look at the definition of Kernel Density Estimation:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$
D_h(x; {x_i}) = \sum_{i=1}^n \frac{1}{nh} K\left(\frac{x - x_i}{h}\right)
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Breaking down this formula a bit: The kernel is the function shown above as &lt;code&gt;$K$&lt;/code&gt; and Janert describes it like so:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To form a KDE, we place a &lt;em&gt;kernel&lt;/em&gt; —that is, a smooth, strongly peaked function—at the
position of each data point. We then add up the contributions from all kernels to obtain a
smooth curve, which we can evaluate at any point along the &lt;em&gt;x&lt;/em&gt; axis&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We are effectively calculating weighted distances from our data points to points along the &lt;em&gt;x&lt;/em&gt; axis. There is a great interactive introduction to kernel density estimation &lt;a href=&#34;https://mathisonian.github.io/kde/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I highly recommend it because you can play with bandwidth, select different kernel methods, and check out the resulting effects.&lt;/p&gt;

&lt;p&gt;As I mentioned before, the default &lt;code&gt;kernel&lt;/code&gt; for this package is the Normal (or Gaussian) probability density function (pdf):&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$$
K(x) = \frac{1}{\sqrt{2\pi}}\text{exp}\left(-\frac{1}{2}x^2\right)
$$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Since we are calculating pdfs I&amp;rsquo;ll use the &lt;a href=&#34;https://github.com/JuliaStats/Distributions.jl&#34; target=&#34;_blank&#34;&gt;Distributions.jl&lt;/a&gt; package to create each distribution, calculate the densities, and sum the results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions

dists = Normal.(x, sqrt(2.25))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;6-element Array{Distributions.Normal{Float64},1}:
 Distributions.Normal{Float64}(μ=-2.1, σ=1.5)
 Distributions.Normal{Float64}(μ=-1.3, σ=1.5)
 Distributions.Normal{Float64}(μ=-0.4, σ=1.5)
 Distributions.Normal{Float64}(μ=1.9, σ=1.5)
 Distributions.Normal{Float64}(μ=5.1, σ=1.5)
 Distributions.Normal{Float64}(μ=6.2, σ=1.5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we see a neat feature of the Julia language. Any Julia function can be vectorized (or broadcasted) by the application of the &lt;code&gt;.&lt;/code&gt; (or &amp;ldquo;dot&amp;rdquo;) operator. See &lt;a href=&#34;https://julialang.org/blog/2018/05/extensible-broadcast-fusion&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; blog post if you want to learn more about it. Above we applied the &lt;code&gt;Normal&lt;/code&gt; method element-wise creating an array of Normal distributions. The mean of our individual distributions being our data points and a variance of &lt;code&gt;2.25&lt;/code&gt; (aka our chosen bandwidth). Let&amp;rsquo;s plot each of these distributions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(dists, legend = false)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_4_1.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Summing up their probability densities across all of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# create an iterator
x_d = range(-7, 11, length = 100)
# find the kde with a gaussian kernel
dens = sum(pdf.(eachdist, x_d) for eachdist in dists)

plot!(x_d, dens)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_5_1.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The resulting shape of the KDE is identical to the one we first calculated. We could stop here except this is really just a special case where we are using the gaussian kernel. Let&amp;rsquo;s extrapolate a bit so we could use different kernels.&lt;/p&gt;

&lt;h2 id=&#34;kernel-density-from-scratch&#34;&gt;Kernel Density from Scratch&lt;/h2&gt;

&lt;p&gt;To apply a new kernel method we can just write the KDE code from scratch. Below I&amp;rsquo;ve defined the KDE function as &lt;code&gt;D&lt;/code&gt; and the kernel argument as &lt;code&gt;K&lt;/code&gt; to mimic the math above.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# define some kernels:
# gaussian kernel
kgauss(x) = 1/sqrt(2π) * exp(-1/2 * x^2)
# boxcar
kbox(x) = abs(x) &amp;lt;= 1 ? 1/2 : 0
# triangular
ktri(x) = abs(x) &amp;lt;= 1 ? 1 - abs(x) : 0

# define the KDE function
D(x, h, xi, K) =
  1/(length(xi) * h) * sum(K.((x .- xi) / h))

# evaluate KDE along the x-axis using comprehensions
dens = [D(xstep, sqrt(2.25), x, K) for xstep in x_d, K in (kgauss, kbox, ktri)]

# visualize the kernels
plot(x_d, dens, label = [&amp;quot;Gaussian&amp;quot;, &amp;quot;Box&amp;quot;, &amp;quot;Triangular&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_6_1.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In my example above I used some shorthand Julia syntax. The &lt;code&gt;?:&lt;/code&gt; syntax is called a ternary operator and makes a conditional &lt;code&gt;if-else&lt;/code&gt; statement more compact. I also used Julia&amp;rsquo;s &lt;code&gt;Assignment&lt;/code&gt; form for my function definitions above because it looks a lot more like the math involved. You could have easily defined each of these functions to look more like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function foo(x)
  if ...
    ...
  else
    ...
  end
  return ...
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;

&lt;p&gt;A short post on cumulative distribution functions (cdf) using Julia will likely follow this one. Janert introduces both kdes and cdfs in his chapter &lt;strong&gt;A Single Variable: Shape and Distribution&lt;/strong&gt; and they complement each other really well. Thanks for reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Homework 2, Chapter 4</title>
      <link>/post/hw2-ch-4/</link>
      <pubDate>Sun, 10 Mar 2019 21:13:14 -0500</pubDate>
      
      <guid>/post/hw2-ch-4/</guid>
      <description>


&lt;div id=&#34;hard-problems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hard Problems&lt;/h2&gt;
&lt;div id=&#34;h1.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4H1.&lt;/h3&gt;
&lt;p&gt;The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(knitr)
library(kableExtra)

kung &amp;lt;- tibble::tribble(
  ~individual, ~weight,
            1,   46.95,
            2,   43.72,
            3,   64.78,
            4,   32.59,
            5,   54.63
  )

# kable(kung, digits = 3, row.names = FALSE, align = &amp;quot;c&amp;quot;, caption = NULL) %&amp;gt;%
#   kable_styling(bootstrap_options = &amp;quot;striped&amp;quot;, full_width = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well it looks like first we need to estimate this model. In the book on page &lt;code&gt;96&lt;/code&gt; we see the MAP estimation of this model on the filtered dataset. I’m going to fit this one for predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rethinking)
data(&amp;quot;Howell1&amp;quot;)

d &amp;lt;- Howell1
d2 &amp;lt;- d[d$age &amp;gt;= 18, ]

# fit model
m4.3 &amp;lt;- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma), # likelihood
    mu &amp;lt;- a + b * weight, # linear model
    a ~ dnorm(156, 100), # prior
    b ~ dnorm(0, 10), # prior
    sigma ~ dunif(0, 50) # prior
  ),
  data = d2
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the model is fit and we can estimate the predictions in two ways:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Collect posterior samples of the parameters of the estimated model and for each weight in the above dataframe sample from a Normal distribution with our posterior samples as parameters&lt;/li&gt;
&lt;li&gt;Use the convenience function in the &lt;code&gt;rethinking&lt;/code&gt; package called &lt;code&gt;sim()&lt;/code&gt; to do the above giving it only the new weights&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- extract.samples(m4.3)
head(post)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          a         b    sigma
## 1 112.0734 0.9424389 5.235063
## 2 112.0144 0.9380622 5.386037
## 3 112.2121 0.9355411 5.176812
## 4 113.2801 0.9155520 5.131002
## 5 112.9359 0.9258103 4.965042
## 6 114.4667 0.8890500 5.054368&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.height &amp;lt;- sapply(kung$weight, function(weight)
  rnorm(
    n = nrow(post),
    mean = post$a + post$b * weight,
    sd = post$sigma
    )
  )

(sim.PI &amp;lt;- apply(sim.height, 2, PI, prob = 0.89))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         [,1]     [,2]     [,3]     [,4]     [,5]
## 5%  148.1218 145.3271 164.2434 135.3212 155.2902
## 94% 164.5610 161.6145 180.6696 151.4692 171.4851&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the convenient way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.height &amp;lt;- sim(m4.3, data = list(weight = kung$weight))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sim.PI &amp;lt;- apply(sim.height, 2, PI, prob = 0.89))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         [,1]     [,2]     [,3]     [,4]     [,5]
## 5%  148.5534 144.9224 163.5080 134.9054 155.4800
## 94% 164.3544 161.6533 180.2119 151.1621 171.0385&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have relatively the same results here which is expected. Let’s get the mean values as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.mean &amp;lt;- apply(sim.height, 2, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s put all of this in the dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kung$exp_height &amp;lt;- sim.mean
kung$lo_89 &amp;lt;- sim.PI[1,]
kung$hi_89 &amp;lt;- sim.PI[2,]

# kable(kung, digits = 3, row.names = FALSE, align = &amp;quot;c&amp;quot;, caption = NULL) %&amp;gt;%
#   kable_styling(bootstrap_options = &amp;quot;striped&amp;quot;, full_width = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;h2.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4H2.&lt;/h2&gt;
&lt;p&gt;Select out all the rows in the &lt;code&gt;Howell1&lt;/code&gt; data with ages below 18 years of age. If you do it right you should end up with a new data frame with 192 rows in it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d3 &amp;lt;- d[d$age &amp;lt; 18,]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;a&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;(a)&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Fit a linear regression to these data, using &lt;code&gt;map&lt;/code&gt;. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma), # likelihood
    mu &amp;lt;- a + b * weight, # linear model
    a ~ dnorm(156, 100), # prior
    b ~ dnorm(0, 10), # prior
    sigma ~ dunif(0, 50) # prior
  ),
  data = d3
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the results of this model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_early &amp;lt;- extract.samples(fit)
precis(post_early)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Mean StdDev |0.89 0.89|
## a     58.26   1.40 56.11 60.58
## b      2.72   0.07  2.61  2.83
## sigma  8.44   0.43  7.72  9.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we see for every &lt;code&gt;1&lt;/code&gt; unit increase in weight a child should be &lt;code&gt;2.72&lt;/code&gt; cm taller. Or as the question has asked &lt;code&gt;10&lt;/code&gt; unit increase is a &lt;code&gt;27.2&lt;/code&gt; cm increase in height. For every &lt;code&gt;22&lt;/code&gt; lb increase there is a corresponding ~&lt;code&gt;1&lt;/code&gt; foot increase in height.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;(b)&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(height ~ weight, d3, col = col.alpha(rangi2, 0.5))

# get index for calculations
weight.seq &amp;lt;- seq(0, 50, by = 1)

# extract samples of parameters and calculate expected mu
mu &amp;lt;- link(fit, data = data.frame(weight = weight.seq))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu.mean &amp;lt;- apply(mu, 2, mean)
lines(weight.seq, mu.mean)
mu.HPDI &amp;lt;- apply(mu, 2, HPDI)
shade(mu.HPDI, weight.seq)

# simulate posterior observations of the model fit
sim.height &amp;lt;- sim(fit, data = data.frame(weight = weight.seq))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;height.HPDI &amp;lt;- apply(sim.height, 2, HPDI)
shade(height.HPDI, weight.seq)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/hw2-ch-4_files/figure-html/unnamed-chunk-10-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;(c)&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well the model appears to be doing good job with the data at hand. The prior’s were set for persons age &amp;gt; 18 so these should be adjusted. Something of conrern is that you see a non-linear structure in this data. So the two tails of the data are not covered well by the model. This non-linearity could be accounted for with a polynomial regression or with splines.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;h3.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4H3.&lt;/h2&gt;
&lt;p&gt;Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the &lt;strong&gt;logarithm&lt;/strong&gt; of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.&lt;/p&gt;
&lt;div id=&#34;a-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;(a)&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Model the relationship between height(cm) and the natural logarithm of weight (log-kg). Use the entire &lt;code&gt;Howell1&lt;/code&gt; data frame, all 544 rows, adults and non-adults. Fit this model, using quadratic approximation:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
h_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta \text{log}(w_i) \\
\alpha \sim \text{Normal}(178, 100) \\
\beta \sim \text{Normal}(0, 100) \\
\sigma \sim \text{Uniform}(0, 50)
\end{aligned}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(h_i\)&lt;/span&gt; is the height of individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt; is the weight (in kg) of individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. The function for computing a natural log in R is just &lt;code&gt;log&lt;/code&gt;. Can you interpret the resulting estimates?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_all &amp;lt;- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu &amp;lt;- a + b * log(weight), # linear model
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 100),
    sigma ~ dunif(0, 50)
  ),
  data = d
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the model is fit let’s review the coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;precis(fit_all)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Mean StdDev   5.5%  94.5%
## a     -23.78   1.34 -25.92 -21.65
## b      47.07   0.38  46.46  47.69
## sigma   5.13   0.16   4.89   5.38&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(precis(fit_all))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/hw2-ch-4_files/figure-html/unnamed-chunk-12-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When the weight is low or equivalent to &lt;code&gt;1&lt;/code&gt; then the height would be &lt;code&gt;-23&lt;/code&gt; which is not insightful. Given that this is a log-level model we can &lt;a href=&#34;https://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor&#34;&gt;interpret&lt;/a&gt; our &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; as a one percent increase in weight corresponds to a &lt;span class=&#34;math inline&#34;&gt;\(\beta/100 = 47.08/100\)&lt;/span&gt; unit increase in height.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;(b)&lt;/h3&gt;
&lt;p&gt;Begin with this plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(height ~ weight, data = d, col = col.alpha(rangi2, 0.4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/hw2-ch-4_files/figure-html/unnamed-chunk-13-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then use the samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% HPDI for the mean, and (3) the 97% HPDI for predicted heights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight.seq &amp;lt;- seq(2, 65, by = 1)
all_samp &amp;lt;- link(fit_all, data = data.frame(weight = weight.seq))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_mu &amp;lt;- apply(all_samp, 2, mean)
all_mu.HPDI &amp;lt;- apply(all_samp, 2, HPDI, prob = 0.97)

all_sim &amp;lt;- sim(fit_all, data = data.frame(weight = weight.seq))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_sim.HPDI &amp;lt;- apply(all_sim, 2, HPDI, prob = 0.97)

plot(height ~ weight, data = d, col = col.alpha(rangi2, 0.4))
# the predicted mean height
lines(weight.seq, all_mu)
# the 97% HPDI of the mean
shade(all_mu.HPDI, weight.seq)
# the 97% HPDI for predictions
shade(all_sim.HPDI, weight.seq)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/hw2-ch-4_files/figure-html/unnamed-chunk-14-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0500</pubDate>
      
      <guid>/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 -0500</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Person Re-Identification System For Mobile Devices</title>
      <link>/publication/person-re-id/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 -0500</pubDate>
      
      <guid>/publication/person-re-id/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mobile visual clothing search</title>
      <link>/publication/clothing-search/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 -0500</pubDate>
      
      <guid>/publication/clothing-search/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example-slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/slides/example-slides/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
