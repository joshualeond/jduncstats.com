<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Josh Duncan</title>
    <link>/post/</link>
    <description>Recent content in Posts on Josh Duncan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Mar 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Market Demand with Instrumental Variables</title>
      <link>/post/2019-03-26_market-dem-with-iv/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-03-26_market-dem-with-iv/</guid>
      <description>Overview Skipper Seabold, a well known contributor to the PyData community, recently gave a talk titled “What’s the Science in Data Science?”. His talk presents several methods commonly used in econometrics that could benefit the field of data science if more widely adopted. Some of the methods were:
 Instrumental Variables Matching Difference-in-Differences  From what I gather, these modeling techniques are popular for discovering causal relationships in observational studies.</description>
    </item>
    
    <item>
      <title>Kernel Density Estimation from Scratch</title>
      <link>/post/2019-03-16_kde-scratch/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 -0500</pubDate>
      
      <guid>/post/2019-03-16_kde-scratch/</guid>
      <description>Motivation Why am I writing about KDE&amp;rsquo;s? At the recommendation of Will Kurt&amp;rsquo;s probability blog I&amp;rsquo;ve been reading a great book on data analysis by Philipp K. Janert:
This book has a great intro to KDEs that explain the motivation. My own abbreviated version are that KDEs provide a useful technique to visualize a variables distribution. Visualizing your data is an important step to take early in the data analysis stage.</description>
    </item>
    
    <item>
      <title>Homework 2, Chapter 4</title>
      <link>/post/hw2-ch-4/</link>
      <pubDate>Sun, 10 Mar 2019 21:13:14 -0500</pubDate>
      
      <guid>/post/hw2-ch-4/</guid>
      <description>Hard Problems 4H1. The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.
library(tidyverse) library(knitr) library(kableExtra) kung &amp;lt;- tibble::tribble( ~individual, ~weight, 1, 46.95, 2, 43.72, 3, 64.78, 4, 32.59, 5, 54.63 ) # kable(kung, digits = 3, row.</description>
    </item>
    
  </channel>
</rss>