<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Josh Duncan</title>
    <link>/post/</link>
    <description>Recent content in Posts on Josh Duncan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Nov 2019 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Model building of golf putting with Turing.jl</title>
      <link>/post/2019-11-02_golf-turing/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 -0500</pubDate>
      
      <guid>/post/2019-11-02_golf-turing/</guid>
      <description>First Thing (Disclaimer) This blog post is based on a blog post written by the popular bayesian statistician Andrew Gelman.
Dr. Gelman&amp;rsquo;s post was written in the Stan probabilistic programming language (or PPL). Since this post, there have been a couple other blog posts translating the code to other PPLs. They are excellent and I definitely recommend checking them out. Here&amp;rsquo;s the list of links and my opinion on some of the benefits of reading each:</description>
    </item>
    
    <item>
      <title>Market Demand with Instrumental Variables</title>
      <link>/post/2019-03-26_market-dem-with-iv/</link>
      <pubDate>Tue, 26 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-03-26_market-dem-with-iv/</guid>
      <description>Overview Skipper Seabold, a well known contributor to the PyData community, recently gave a talk titled “What’s the Science in Data Science?”. His talk presents several methods commonly used in econometrics that could benefit the field of data science if more widely adopted. Some of the methods were:
 Instrumental Variables Matching Difference-in-Differences  From what I gather, these modeling techniques are popular for discovering causal relationships in observational studies.</description>
    </item>
    
    <item>
      <title>Kernel Density Estimation from Scratch</title>
      <link>/post/2019-03-16_kde-scratch/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 -0500</pubDate>
      
      <guid>/post/2019-03-16_kde-scratch/</guid>
      <description>Motivation Why am I writing about KDE&amp;rsquo;s? At the recommendation of Will Kurt&amp;rsquo;s probability blog I&amp;rsquo;ve been reading a great book on data analysis by Philipp K. Janert:
This book has a great intro to KDEs that explain the motivation. My own abbreviated version are that KDEs provide a useful technique to visualize a variables distribution. Visualizing your data is an important step to take early in the data analysis stage.</description>
    </item>
    
  </channel>
</rss>