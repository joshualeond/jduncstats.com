[{"authors":["admin"],"categories":null,"content":"Hey, I\u0026rsquo;m Josh Duncan, a former controls engineer that found a passion in data science and statistics. This blog is a platform for extending my data science education. I\u0026rsquo;m currently on a Bayesian modeling journey so you\u0026rsquo;ll likely see posts on learning bayes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Hey, I\u0026rsquo;m Josh Duncan, a former controls engineer that found a passion in data science and statistics. This blog is a platform for extending my data science education. I\u0026rsquo;m currently on a Bayesian modeling journey so you\u0026rsquo;ll likely see posts on learning bayes.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536469200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536469200,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906567200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906567200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-06:00","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Updates (May 2020): I originally wrote this Turing.jl version of the golf putting model in late 2019. Since then there have been many updates to the Turing.jl Julia package. I\u0026rsquo;ve updated this blog post to be aligned with the latest release (v0.13.0) and have also included something new given that the latest release includes a feature for sampling from the prior distribution.\n First Thing (Disclaimer) This blog post is based on a blog post written by the popular bayesian statistician Andrew Gelman.\nDr. Gelman\u0026rsquo;s post was written in the Stan probabilistic programming language (or PPL). Since this post, there have been a couple other blog posts translating the code to other PPLs. They are excellent and I definitely recommend checking them out. Here\u0026rsquo;s the list of links and my opinion on some of the benefits of reading each:\n Dr. Gelman\u0026rsquo;s using Stan with R: See here. Read Dr. Gelman\u0026rsquo;s post first as he lays out the problem being modeled here really well. Explaining the geometry at play and how to adjust the models to incorporate the physics of the problem. It\u0026rsquo;s a great article showing the power of using a probabilistic programming language to build models. Colin Carroll\u0026rsquo;s using the Python PPL PyMC3: You can check out his work here. I really enjoyed Colin\u0026rsquo;s post because of his prior and posterior predictive checks, data visualizations, and showing the value of these models moving past predictions. Adam Haber\u0026rsquo;s post using Python and Tensorflow Probability: See here. His post goes into great detail about more of the sampling and lower level details of Tensorflow Probability and MCMC.  My post is nothing really novel, simply a port to the Julia PPL, Turing.jl. This post shows how to specify and estimate these same models within the Julia programming language.\nGetting started and plotting Let\u0026rsquo;s Load the data and take a look:\nusing Turing, HTTP, CSV # Hide sampling progress. Turing.turnprogress(false); resp = HTTP.get(\u0026quot;https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/golf/golf_data.txt\u0026quot;); data = CSV.read(IOBuffer(resp.body), normalizenames = true, header = 3) x, n, y = (data[:,1], data[:,2], data[:,3])  We have three variables in this dataset:\n   Variable Units Description     x feet The distance of the attempted putt   n count The total attempts (or trials) of putts at a chosen distance   y count The total successful putts from the total attempts    What we are attempting to build is a model to predict the probability of success given the distance from the hole. We need to transform this data a bit to explore the dataset visually. Let\u0026rsquo;s calculate the probabilities and the error involved. We\u0026rsquo;ll use the following formula to calculate the error bars for the putting success rate:\n$$ \\sqrt{\\hat{p}_j(1-\\hat{p}_j)/n_j} $$\npj = y ./ n error = @. sqrt((pj * (1 - pj) / n));  Now let\u0026rsquo;s visualize the dataset:\nusing Plots # plot out the error scatter( x, pj, yerror= error, legend = false, ylim = (0, 1), ylab = \u0026quot;Probability of Success\u0026quot;, xlab = \u0026quot;Distance from hole (ft)\u0026quot;)  Logistic Regression Building our first model, a GLM (Generalized Linear Model). We will attempt to model the probability of success in golf putting incorporating the distance as an independent (or predictor) variable.\n$$ y_j\\sim\\mbox{binomial}(n_j, \\mbox{logit}^{-1}(a + bx_j)), \\mbox{ for } j=1,\\dots, J. $$\nusing StatsFuns: logistic @model golf_logistic(x,y,n,J) = begin # parameters a ~ Normal(0, 1) b ~ Normal(0, 1) # model for i in 1:J p = logistic(a + b * x[i]) y[i] ~ Binomial(n[i], p) end end chn = sample(golf_logistic(x, y, n, length(x)), NUTS(), MCMCThreads(), 4000, 4);  The sample method now allows for parallel sampling with either one thread per chain (MCMCThreads) or one process per chain (MCMCDistributed). Here I\u0026rsquo;m using multithreading and am specifying 4 chains should be used.\nNow that we\u0026rsquo;ve sampled the joint probability distribution. Let\u0026rsquo;s take a look at the results. I\u0026rsquo;ll create a function to show the table of results as html using the PrettyTables.jl package. Looking at the summary statistics for the posterior distribution:\nusing PrettyTables, DataFrames formatters = (v,i,j) -\u0026gt; (j \u0026gt; 1) ? round(v, digits=3) : v function prettystats(chains) chains |\u0026gt; x -\u0026gt; summarystats(x) |\u0026gt; x -\u0026gt; DataFrame(x) |\u0026gt; x -\u0026gt; pretty_table(x, backend = :html, formatters = formatters) end prettystats(chn)    table, td, th { border-collapse: collapse; font-family: sans-serif; } td, th { border-bottom: 0; padding: 4px }\ntr:nth-child(odd) { background: #eee; }\ntr:nth-child(even) { background: #fff; }\ntr.header { background: navy !important; color: white; font-weight: bold; }\ntr.subheader { background: lightgray !important; color: black; }\ntr.headerLastRow { border-bottom: 2px solid black; }\nth.rowNumber, td.rowNumber { text-align: right; }\n\n  parameters mean std naive_se mcse ess r_hat  String Float64 Float64 Float64 Float64 Float64 Float64   a 2.225 0.057 0.001 0.001 2997.531 1.002   b -0.255 0.007 0.0 0.0 3137.058 1.002     Visualizing the predictions:\na_post = median(chn[:a].value) b_post = median(chn[:b].value) # iterator for distance from hole calcs xrng = 1:1:21 post_lines = [logistic(a_post + b_post * x) for x = xrng] # 50 draws from the posterior using StatsBase a_samp = StatsBase.sample(chn[:a].value, 50) b_samp = StatsBase.sample(chn[:b].value, 50) post_samp = [logistic(a_samp[i] + b_samp[i] * x) for x = xrng, i = 1:50] plot!(post_samp, alpha = 0.5, color = :gray) # add uncertainty samples plot!(post_lines, color = :black) # add median  First Principles The next step is building a more bespoke model that incorporates the physics of this problem. Dr. Gelman describes this step as follows:\n We assume that the golfer is attempting to hit the ball completely straight but that many small factors interfere with this goal, so that the actual angle follows a normal distribution centered at 0 with some standard deviation σ.\n  The probability the ball goes in the hole is then the probability that the angle is less than the threshold; that is $$ \\mbox{Pr}\\left(|\\mbox{angle}| \u0026lt; \\sin^{-1}((R-r)/x)\\right) = 2\\Phi\\left(\\frac{\\sin^{-1}((R-r)/x)}{\\sigma}\\right) - 1 $$\n  where Φ is the cumulative normal distribution function.\n Again, for more background I would suggest reading the original post. So now we\u0026rsquo;ll define the function Phi. It\u0026rsquo;s the cumulative distribution function of the standard normal distribution.\nPhi(x) = cdf.(Normal(0, 1), x);  Now let\u0026rsquo;s create and sample this model incorporating the angle.\n@model golf_angle(x, y, n, J, r, R) = begin # transformed data threshold_angle = asin.((R - r) ./ x) # parameters sigma ~ truncated(Normal(0, 1), 0, Inf) # model p = 2 * Phi(threshold_angle / sigma) .- 1 for i in 1:J y[i] ~ Binomial(n[i], p[i]) end end # radius of ball and hole respectively r = (1.68 / 2) / 12 R = (4.25 / 2) / 12  Taking a note from Colin\u0026rsquo;s book, we can perform prior predictive checks for the geometry-based model. In Turing we\u0026rsquo;ll simply replace the sampler argument on the sample method with Prior().\nprior = sample(golf_angle(x, y, n, length(x), r, R), Prior(), 4000) angle_prior = StatsBase.sample(prior[:sigma].value, 500) angle_of_shot = rand.(Normal.(0, angle_prior), 1) # radians angle_of_shot = getindex.(angle_of_shot) # extract array distance = 20 # feet end_positions = [ distance * cos.(angle_of_shot), distance * sin.(angle_of_shot) ] # visualize plot( [[0, i] for i in end_positions[1]], [[0, i] for i in end_positions[2]], labels = false, legend = :topleft, color = :black, alpha = 0.3, title = \u0026quot;Prior distribution of putts from 20ft away\u0026quot; ) scatter!(end_positions[1], end_positions[2], color = :black, labels = false) scatter!((0,0), color = :green, label = \u0026quot;start\u0026quot;, markersize = 6) scatter!((20, 0), color = :red, label = \u0026quot;goal\u0026quot;, markersize = 6)  Now for the estimation of the parameters:\nchn2 = sample(golf_angle(x, y, n, length(x), r, R), NUTS(), MCMCThreads(), 4000, 4) chn2 = hcat(chn2, Chains(chn2[:sigma].value * 180 / π, [\u0026quot;sigma_degrees\u0026quot;])) prettystats(chn2)    table, td, th { border-collapse: collapse; font-family: sans-serif; } td, th { border-bottom: 0; padding: 4px }\ntr:nth-child(odd) { background: #eee; }\ntr:nth-child(even) { background: #fff; }\ntr.header { background: navy !important; color: white; font-weight: bold; }\ntr.subheader { background: lightgray !important; color: black; }\ntr.headerLastRow { border-bottom: 2px solid black; }\nth.rowNumber, td.rowNumber { text-align: right; }\n\n  parameters mean std naive_se mcse ess r_hat  String Float64 Float64 Float64 Float64 Float64 Float64   sigma 0.027 0.0 0.0 0.0 5818.38 1.0   sigma_degrees 1.527 0.023 0.0 0.0 5818.38 1.0     Now we can calculate predictions and see how this model compares to the logistic model. Let\u0026rsquo;s wrap the angle calculation into a function as we\u0026rsquo;ll use it frequently throughout the post.\nprob_angle(threshold, sigma) = 2 * Phi(threshold / sigma) .- 1  Calculate and visualize predictions.\n# calculate predictions post_sigma = median(chn2[:sigma].value) threshold_angle = [asin((R - r) / x) for x = xrng] geom_lines = prob_angle(threshold_angle, post_sigma) scatter( x, pj, yerror= error, label = \u0026quot;\u0026quot;, ylim = (0, 1), ylab = \u0026quot;Probability of Success\u0026quot;, xlab = \u0026quot;Distance from hole (ft)\u0026quot;) plot!(post_lines, color = :black, label = \u0026quot;Logistic regression\u0026quot;) plot!(geom_lines, color = 1, label = \u0026quot;Geometry-based model\u0026quot;)  We see that the geometry based model fits the data much better than the Logistic regression.\nNow let\u0026rsquo;s perform a posterior predictive check:\nangle_post = StatsBase.sample(chn2[:sigma].value, 500) angle_of_shot = rand.(Normal.(0, angle_post), 1) # radians angle_of_shot = getindex.(angle_of_shot) # extract array distance = 20 # feet end_positions = [ distance * cos.(angle_of_shot), distance * sin.(angle_of_shot) ] # visualize plot( [[0, i] for i in end_positions[1]], [[0, i] for i in end_positions[2]], xlim = (-21, 21), ylim = (-21, 21), labels = false, legend = :topleft, color = :black, alpha = 0.1, title = \u0026quot;Posterior distribution of putts from 20ft away\u0026quot; ) scatter!(end_positions[1], end_positions[2], color = :black, labels = false) scatter!((0,0), color = :green, label = \u0026quot;start\u0026quot;, markersize = 6) scatter!((20, 0), color = :red, label = \u0026quot;goal\u0026quot;, markersize = 6)  New Golf Data Then came new data. Dr. Gelman received new data of golf putting and compared the fit of the original geometry based model with the old and new data. Here is the comparison below:\nrespnew = HTTP.get(\u0026quot;https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/golf/golf_data_new.txt\u0026quot;); datanew = CSV.read(IOBuffer(respnew.body), normalizenames = true, header = 3) xnew, nnew, ynew = (datanew[:,1], datanew[:,2], datanew[:,3]) pnew = ynew ./ nnew xrngnew = 1:1:80 # plot the old model fit with new data threshold_angle2 = [asin((R - r) / x) for x = xrngnew] geom_lines2 = prob_angle(threshold_angle2, post_sigma) scatter( x, pj, label = \u0026quot;Old data\u0026quot;, ylab = \u0026quot;Probability of Success\u0026quot;, xlab = \u0026quot;Distance from hole (ft)\u0026quot;, color = 1) scatter!(xnew, pnew, color = 2, label = \u0026quot;New data\u0026quot;) plot!(geom_lines2, label = \u0026quot;\u0026quot;, color = 1)  We see that the new data have many more observations with longer distance putts than in the original data. The probability of success for these longer distance putts do not agree with the original geometry based model.\nUpdated Geometry So Dr. Gelman improves the model by taking distance into account.\n To get the ball in the hole, the angle isn’t the only thing you need to control; you also need to hit the ball just hard enough.\n  \u0026hellip;the probability a shot goes in becomes, $$ \\left(2\\Phi\\left(\\frac{\\sin^{-1}((R-r)/x)}{\\sigma_{\\rm angle}}\\right) - 1\\right)\\left(\\Phi\\left(\\frac{2}{(x+1),\\sigma_{\\rm distance}}\\right) - \\Phi\\left(\\frac{-1}{(x+1),\\sigma_{\\rm distance}}\\right)\\right) $$\n  where we have renamed the parameter σ from our earlier model to σ_angle to distinguish it from the new σ_distance parameter.\n Let\u0026rsquo;s add a function for the distance calculation for the improved geometry.\nprob_distance(distance, tol, overshot, sigma) = Phi((tol - overshot) ./ ((distance .+ overshot) * sigma)) - Phi(-overshot ./ ((distance .+ overshot) * sigma));  Now let\u0026rsquo;s create the model to sample:\n@model golf_angle_dist(x, y, n, J, r, R, overshot, distance_tolerance) = begin # transformed data threshold_angle = asin.((R - r) ./ x) # parameters sigma_angle ~ truncated(Normal(0, 1), 0, Inf) sigma_distance ~ truncated(Normal(0, 1), 0, Inf) # model p_angle = prob_angle(threshold_angle, sigma_angle) p_distance = prob_distance(x, distance_tolerance, overshot, sigma_distance) p = p_angle .* p_distance for i in 1:J y[i] ~ Binomial(n[i], p[i]) end end overshot = 1. distance_tolerance = 3. chn3 = sample( golf_angle_dist(xnew, ynew, nnew, length(xnew), r, R, overshot, distance_tolerance), NUTS(), MCMCThreads(), 6000, 4) prettystats(chn3)    table, td, th { border-collapse: collapse; font-family: sans-serif; } td, th { border-bottom: 0; padding: 4px }\ntr:nth-child(odd) { background: #eee; }\ntr:nth-child(even) { background: #fff; }\ntr.header { background: navy !important; color: white; font-weight: bold; }\ntr.subheader { background: lightgray !important; color: black; }\ntr.headerLastRow { border-bottom: 2px solid black; }\nth.rowNumber, td.rowNumber { text-align: right; }\n\n  parameters mean std naive_se mcse ess r_hat  String Float64 Float64 Float64 Float64 Float64 Float64   sigma_angle 0.014 0.002 0.0 0.0 80.321 1.317   sigma_distance 0.135 0.01 0.0 0.001 80.321 1.36     Dr. Gelman\u0026rsquo;s post suggests something is unstable with this model estimation. The estimated parameters match his closely. I did notice that sampling this model did not always give me same results though. I ended up increasing the length of the chains to try to get around the inconsistent parameter estimation I was experiencing.\nNow let\u0026rsquo;s make some predictions and visualize the results:\n# calculate predictions post_siga = median(chn3[:sigma_angle].value) post_sigd = median(chn3[:sigma_distance].value) p_angle = prob_angle(threshold_angle2, post_siga) p_distance = prob_distance(xrngnew, distance_tolerance, overshot, post_sigd) geom2_lines = p_angle .* p_distance # plot scatter( xnew, pnew, legend = false, color = 2, ylab = \u0026quot;Probability of Success\u0026quot;, xlab = \u0026quot;Distance from hole (ft)\u0026quot;) plot!(geom2_lines, color = 2)  Now this model fits the new data better than the original geometry based model but you can see an issue near the middle of the range of the x-axis (distance). Some of Gelman\u0026rsquo;s select comments and proposed fix:\n There are problems with the fit in the middle of the range of x. We suspect this is a problem with the binomial error model, as it tries harder to fit points where the counts are higher. Look at how closely the fitted curve hugs the data at the very lowest values of x.\n  To fix this problem we took the data model, $ y_j \\sim \\mbox{binomial}(n_j, p_j) $, and added an independent error term to each observation.\n  \u0026hellip;we first approximate the binomial data distribution by a normal and then add independent variance; thus: $$ y_j/n_j \\sim \\mbox{normal}\\left(p_j, \\sqrt{p_j(1-p_j)/n_j + \\sigma_y^2}\\right) $$\n A Dispersed Model Now let\u0026rsquo;s implement the changes referenced above within Turing:\n@model golf_angle_dist_resid(x, y, n, J, r, R, overshot, distance_tolerance, raw) = begin # transformed data threshold_angle = asin.((R - r) ./ x) # parameters sigma_angle ~ truncated(Normal(0, 1), 0, Inf) sigma_distance ~ truncated(Normal(0, 1), 0, Inf) sigma_y ~ truncated(Normal(0, 1), 0, Inf) # model p_angle = prob_angle(threshold_angle, sigma_angle) p_distance = prob_distance(x, distance_tolerance, overshot, sigma_distance) p = p_angle .* p_distance for i in 1:J raw[i] ~ Normal(p[i], sqrt(p[i] * (1-p[i]) / n[i] + sigma_y^2)) end end chn4 = sample( golf_angle_dist_resid(xnew, ynew, nnew, length(xnew), r, R, overshot, distance_tolerance, ynew ./ nnew), NUTS(), MCMCThreads(), 4000, 4) # adding the conversion to degrees chns = hcat(chn4, Chains(chn4[:sigma_angle].value * 180 / π, [\u0026quot;sigma_degrees\u0026quot;])) prettystats(chns)    table, td, th { border-collapse: collapse; font-family: sans-serif; } td, th { border-bottom: 0; padding: 4px }\ntr:nth-child(odd) { background: #eee; }\ntr:nth-child(even) { background: #fff; }\ntr.header { background: navy !important; color: white; font-weight: bold; }\ntr.subheader { background: lightgray !important; color: black; }\ntr.headerLastRow { border-bottom: 2px solid black; }\nth.rowNumber, td.rowNumber { text-align: right; }\n\n  parameters mean std naive_se mcse ess r_hat  String Float64 Float64 Float64 Float64 Float64 Float64   sigma_angle 0.018 0.0 0.0 0.0 5026.215 1.001   sigma_distance 0.08 0.001 0.0 0.0 4901.694 1.0   sigma_y 0.003 0.001 0.0 0.0 6563.99 1.0   sigma_degrees 1.02 0.006 0.0 0.0 5026.215 1.001     Calculate predictions and visualize:\npost_siga = median(chn4[:sigma_angle].value) post_sigd = median(chn4[:sigma_distance].value) p_angle2 = prob_angle(threshold_angle2, post_siga) p_distance2 = prob_distance(xrngnew, distance_tolerance, overshot, post_sigd) geom_lines2 = p_angle2 .* p_distance2 # plot scatter( xnew, pnew, legend = false, color = 2, ylab = \u0026quot;Probability of Success\u0026quot;, xlab = \u0026quot;Distance from hole (ft)\u0026quot;) plot!(geom_lines2, color = 2)  We can see that this adjusted first principles based model is fitting the data much better now! To add to that, it also sampled faster and more consistently during my testing. This case study really shows off the power of a bayesian approach. The modeler has the ability to expand a model using domain knowledge and craft a model that makes sense and aligns with the data generating process.\nConclusion If you made it this far, thanks for checking out this post! I personally appreciate all of the great scientists, applied statisticians, etc. that have created and shared the other posts I referenced as well as the team developing the Turing PPL within Julia. It\u0026rsquo;s really exciting to see a PPL written entirely in one language. In my opinion, it shows the strengths of the Julia language and is an example of its promise to solve the two-language problem.\nReferences   Stan: a state-of-the-art platform for statistical modeling and high-performance statistical computation  Model building and expansion for golf putting - Gelman  Model building and expansion for golf putting - Carroll  Bayesian golf puttings, NUTS, and optimizing your sampling function with TensorFlow Probability  Turing.jl: A library for robust, efficient, general-purpose probabilistic programming  Announcing composable multi-threaded parallelism in Julia  ODSC East 2016 | Stefan Karpinski - \u0026ldquo;Solving the Two Language Problem\u0026rdquo;  ","date":1590382800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590382800,"objectID":"26eceba2f000e78faedb3d8b09ac5634","permalink":"/post/2019-11-02_golf-turing/","publishdate":"2020-05-25T00:00:00-05:00","relpermalink":"/post/2019-11-02_golf-turing/","section":"post","summary":"Updates (May 2020): I originally wrote this Turing.jl version of the golf putting model in late 2019. Since then there have been many updates to the Turing.jl Julia package. I\u0026rsquo;ve updated this blog post to be aligned with the latest release (v0.","tags":["julia","bayes","Turing.jl"],"title":"Model building of golf putting with Turing.jl","type":"post"},{"authors":null,"categories":[],"content":" Overview Skipper Seabold, a well known contributor to the PyData community, recently gave a talk titled “What’s the Science in Data Science?”. His talk presents several methods commonly used in econometrics that could benefit the field of data science if more widely adopted. Some of the methods were:\n Instrumental Variables Matching Difference-in-Differences  From what I gather, these modeling techniques are popular for discovering causal relationships in observational studies. When an RCT (randomized control trial) is unreasonable then these techniques give us an alternative approach.\nI’ll be exploring one of these methods called Instrumental Variables (or IV). An example use case from Skipper’s talk was a study on the Fulton Fish Market in NYC.\nThe referenced study estimates the demand curve for fish at the market. Finding the demand curve is unfortunately not as simple as regressing quantity on price. The relationship between price and the quantity of fish sold is not exclusive to demand but includes supply effects at the market. We’ll be using IV to account for supply effects to isolate the demand effects.\nIn this blog post I’ll be reproducing a portion of this analysis using R, packages of the tidyverse, and the brms package in R.\n Data I discovered data related to this study at the following website: http://people.brandeis.edu/~kgraddy/data.html\nBringing this data into R is very simple. The linked dataset appears to be the cleaned and transformed data used within the paper. Let’s read it into our R environment and take a look:\nlibrary(tidyverse) fulton \u0026lt;- read_tsv(\u0026quot;http://people.brandeis.edu/~kgraddy/datasets/fish.out\u0026quot;) fulton ## # A tibble: 111 x 16 ## day1 day2 day3 day4 date stormy mixed price qty rainy cold ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0 0 0 911202 1 0 -0.431 8.99 1 0 ## 2 0 1 0 0 911203 1 0 0 7.71 0 0 ## 3 0 0 1 0 911204 0 1 0.0723 8.35 1 1 ## 4 0 0 0 1 911205 1 0 0.247 8.66 0 1 ## 5 0 0 0 0 911206 1 0 0.664 7.84 0 1 ## 6 1 0 0 0 911209 0 0 -0.207 9.30 0 0 ## 7 0 1 0 0 911210 0 1 -0.116 8.92 0 0 ## 8 0 0 1 0 911211 0 0 -0.260 9.11 1 0 ## 9 0 0 0 1 911212 0 1 -0.117 8.31 0 0 ## 10 0 0 0 0 911213 0 0 -0.342 9.21 0 0 ## # … with 101 more rows, and 5 more variables: windspd \u0026lt;dbl\u0026gt;, ## # windspd2 \u0026lt;dbl\u0026gt;, pricelevel \u0026lt;dbl\u0026gt;, totr \u0026lt;dbl\u0026gt;, tots \u0026lt;dbl\u0026gt; Here is a quick description of the variables we will be using for this work:\n  Variable Units Description    qty log(pounds) The total amount of fish sold on a day  price log($/lb) Average price for the day  day1-day4 dummy var Monday-Thurs  cold dummy var Weather on shore  rainy dummy var Rain on shore  stormy dummy var Wind and waves off shore (a 3-day moving average)    The paper informs the reader that transactions recorded are of a particular fish species called Whiting. We can get a feel for the total amount of Whiting being sold by reproducing Figure 2 from the paper.\n# fixing the date column fulton %\u0026gt;% mutate( date = as.character(date), date = parse_date(date, format = \u0026quot;%y%m%d\u0026quot;) ) %\u0026gt;% ggplot(aes(x = date, y = exp(qty))) + geom_col() + labs( title = \u0026quot;Figure 2\u0026quot;, subtitle = \u0026quot;Daily Volumes of Whiting\u0026quot;, x = \u0026quot;Date (December 2, 1991-May 8, 1992)\u0026quot;, y = \u0026quot;Quantity (pounds)\u0026quot; ) Reproducing this figure gave me confidence that I had the correct data to reproduce the analysis.\n Demand Curve The naive approach to finding the relationship between price and demand (the demand curve) would be to regress quantity on price:\nggplot(fulton, aes(x = price, y = qty)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) But would this be the demand curve? No, it actually wouldn’t be. According to intro economics, each one of the points in the plot above is the result of the intersection of both a supply and demand curve. Something like this figure:\nSo how does the author go about estimating a more accurate representation of the demand curve? How do you isolate the demand effects from the supply? Well, it’s in the title of this post: Instrumental Variables.\nSkipper’s presentation explained IV as:\n We can replace X with “instruments” that are correlated with X but not caused by Y or that affect Y but only through X.\n  Regressions The paper includes a table of estimated coefficients shown here:\nI’ll be reproducing this table but with Bayesian estimation. A nice benefit of using Bayesian estimation is that our estimated model includes distributions for each of our parameters. We’ll be visualizing these distributions for comparing results to the table above.\nOLS (Ordinary Least Squares) Reproduced First, let’s perform the classic linear regression. We can use the brm function in the brms package as a drop in replacement for R’s linear model function lm. However, the estimation process of lm and brm are quite different. The lm function is using OLS and the brms package is performing Bayesian estimation using a form of Markov Chain Monte Carlo (MCMC).\nStarting with column 1 of table 2 we estimate the coefficients with only qty and price:\nlibrary(brms) library(tidybayes) library(ggridges) # column 1 in table 2 fit1 \u0026lt;- brm(qty ~ price, data = fulton, refresh = 0) fit1 %\u0026gt;% posterior_samples() %\u0026gt;% ggplot(aes(x = b_price)) + geom_density_line() The brms package provides an abstraction layer to the Stan probabilistic programming language. So if you’re curious what the “uninformative” priors that I breezed over actually are, take a look at the generated code with the stancode function. Here is an excerpt of the stan code generated for the model above:\nmodel { vector[N] mu = temp_Intercept + Xc * b; // priors including all constants target += student_t_lpdf(temp_Intercept | 3, 9, 10); target += student_t_lpdf(sigma | 3, 0, 10) - 1 * student_t_lccdf(0 | 3, 0, 10); // likelihood including all constants if (!prior_only) { target += normal_lpdf(Y | mu, sigma); } } Moving to column 2 of table 2 we estimate the model including the dummy day variables, cold, and rainy variables:\n# column 2 in table 2 fit1_full \u0026lt;- brm( qty ~ price + day1 + day2 + day3 + day4 + cold + rainy, data = fulton, refresh = 0 ) # a plot of parameter estimates fit1_full %\u0026gt;% posterior_samples() %\u0026gt;% gather(b_price:b_rainy, key = \u0026quot;coef\u0026quot;, value = \u0026quot;est\u0026quot;) %\u0026gt;% ggplot(aes(x = est, y = coef)) + geom_density_ridges() We can see that the estimated coefficient for price in this context is the same for both the simple linear regression and the regression accounting for weekday and onshore weather.\n IV Reproduced Before performing the estimation I wanted to add a quote from the paper. I thought Kathryn Graddy explained the IV estimation well:\n That is, first a regression is run with log price as the dependent variable and the storminess of the weather as the explanatory variable. This regression seeks to measure the variation in price that is attributable to stormy weather. The coefficients from this regression are then used to predict log price on each day, and these predicted values for price are inserted back into the regression.\n Kathryn mentions two steps here:\nA regression with log price and storminess Using coefficients from step 1, predict log price and place the predicted values back into the second regression.  A simple diagram of this two step process is generated below:\nlibrary(ggdag) dagify( qty ~ price, price ~ stormy ) %\u0026gt;% ggdag(seed = 12) + theme(panel.background = element_blank()) Now let’s perform the estimation. This is possible with the brms package and its ability to specify multivariate response models. We’ll piece it together with two separate formulas defined in the bf function calls below.\nThe first estimation will be for column 3 of table 2:\n# measure variation in price attributable to stormy weather fit2a \u0026lt;- bf(price ~ stormy) # estimate demand fit2b \u0026lt;- bf(qty ~ price) # column 3 in table 2 fit2 \u0026lt;- brm(fit2a + fit2b, data = fulton, refresh = 0) fit2 %\u0026gt;% posterior_samples() %\u0026gt;% ggplot(aes(x = b_qty_price)) + geom_density_line() And lastly we’ll estimate the IV with the remaining variables (column 4):\n# visual representation of the estimation dagify( qty ~ price + day1 + day2 + day3 + day4 + cold + rainy, price ~ stormy ) %\u0026gt;% ggdag(seed = 9) + theme(panel.background = element_blank()) # add additional demand specific variables fit2b_full \u0026lt;- bf(qty ~ price + day1 + day2 + day3 + day4 + cold + rainy) # column 4 in table 2 fit2_full \u0026lt;- brm(fit2a + fit2b_full, data = fulton, refresh = 0) fit2_full %\u0026gt;% posterior_samples() %\u0026gt;% gather(b_qty_price:b_qty_rainy, key = \u0026quot;coef\u0026quot;, value = \u0026quot;est\u0026quot;) %\u0026gt;% ggplot(aes(x = est, y = coef)) + geom_density_ridges() One thing that I think is particularly interesting about the distributions above is the uncertainty of the price coefficient. We can clearly see that the parameter’s distribution with IV is much wider than our classic linear regression. Here we are seeing the uncertainty of our first model (price ~ storminess) propagating to our second model.\n Visualizing the Demand Curve Now we can plot the demand curve with isolated demand effects. I’ll use the first IV estimation with only qty, price, and stormy variables:\nfulton %\u0026gt;% add_predicted_draws(fit2) %\u0026gt;% filter(.category == \u0026quot;qty\u0026quot;) %\u0026gt;% ggplot(aes(x = price, y = qty)) + stat_lineribbon( aes(y = .prediction), .width = c(.99, .95, .8, .5), color = \u0026quot;#08519C\u0026quot; ) + geom_point(data = fulton, size = 2) + scale_fill_brewer() The prediction is downward trending as the original curve except with a steeper slope (-0.54 vs -1.09).\n  Elasticities The paper breaks down the interpretation of the estimated coefficients in terms of elasticities. Given that we have taken the log of both our quantity and price the coefficients can be interpreted in a clever way. Let’s take a look at why this is the case:\nIf we take the log of both \\(y\\) and \\(x\\) of our linear model:\n\\[ \\text{log}(y) = \\beta_0 + \\beta_1\\text{log}(x) + \\epsilon \\] Now solve for \\(y\\) to find the marginal effects:\n\\[ y = e^{\\beta_o + \\beta_1\\text{log}(x) + \\epsilon} \\] Then differentiate with respect to \\(x\\):\n\\[ \\frac{dy}{dx} = \\frac{\\beta_1}{x}e^{\\beta_o + \\beta_1\\text{log}(x) + \\epsilon} = \\beta_1 \\frac{y}{x} \\]\nIf you then solve for \\(\\beta_1\\) you find:\n\\[ \\beta_1 = \\frac{dy}{dx} \\frac{x}{y} \\] So here \\(\\beta_1\\) is an elasticity. For a \\(\\%\\) increase in \\(x\\) there is a \\(\\beta_1 \\%\\) increase in \\(y\\).\nElasticities are commonly summarized in a table like this:\n  Elasticity Value Description    Elastic | E | \u0026gt; 1 % change in Q \u0026gt; % change in P  Unitary Elastic | E | = 1 % change in Q = %change in P  Inelastic | E | \u0026lt; 1 % change in Q \u0026lt; % change in P    Given the descriptions of elasticity above. We would have two different interpretations of how demand responds to price with the non-IV and the IV estimation. With the non-IV estimation our elasticity coefficient is -0.54 [-0.9, -0.18]. With the IV estimation our elasticity is -1.09 [-2.17, -0.15]. So we would mistakenly interpret the demand elasticity as being inelastic when it actually appears to be unit elastic.\nSome interesting interpretations of this unit elasticity from the paper include:\n First, it is consistent with pricing power on the part of the fish dealers. A price-setting firm will raise price to the point where the percentage change in the quantity demanded is at least as large as the percentage change in price; otherwise, it would make sense to raise the price even more\n  Second, when demand has a unitary elasticity, it means that the percentage change in quantity would always equal the percentage change in price, and the weather would therefore not have much effect on a seller’s revenue, keeping fishermen’s incomes relatively constant.\n  Third, unit elasticities could also result from budget constraints on the part of some buyers.\n  Wrapping Up This was definitely a fun topic to start exploring. I plan on taking a look at difference-in-differences in the near future as well.\n References  What’s the Science in Data Science? Graddy, Kathryn. 2006. “Markets: The Fulton Fish Market.” Journal of Economic Perspectives, 20 (2): 207-220. Create supply and demand economics curves with ggplot2 Michael Betancourt: “A Conceptual Introduction to Hamiltonian Monte Carlo”, 2017   ","date":1553558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553558400,"objectID":"b3a9dcc87549ab7a181a86ebe6f03722","permalink":"/post/2019-03-26_market-dem-with-iv/","publishdate":"2019-03-26T00:00:00Z","relpermalink":"/post/2019-03-26_market-dem-with-iv/","section":"post","summary":"Overview Skipper Seabold, a well known contributor to the PyData community, recently gave a talk titled “What’s the Science in Data Science?”. His talk presents several methods commonly used in econometrics that could benefit the field of data science if more widely adopted.","tags":["rstats","econometrics","bayes"],"title":"Market Demand with Instrumental Variables","type":"post"},{"authors":null,"categories":null,"content":"Motivation Why am I writing about KDE\u0026rsquo;s? At the recommendation of Will Kurt\u0026rsquo;s probability blog I\u0026rsquo;ve been reading a great book on data analysis by Philipp K. Janert:\nThis book has a great intro to KDEs that explain the motivation. My own abbreviated version are that KDEs provide a useful technique to visualize a variables distribution. Visualizing your data is an important step to take early in the data analysis stage. In fact, there are many metrics that we commonly use to understand a variable that have an implicit assumption that your data are unimodal (having a single peak). If your data doesn\u0026rsquo;t have this structure then you may be mislead by measures of central tendency (mean/median/mode), outliers, or other statistical methods (linear regression, t-tests, etc.).\nThis post\u0026rsquo;s structure follows closely with how I commonly learn topics. I start at a high level, using a pre-canned solution for the algorithm, and then work backward to find out what\u0026rsquo;s going on underneath.\nI use a small example I discovered on the Wikipedia page for KDEs. It uses a handful of data points for a single variable:\n   Sample Value     1 -2.1   2 -1.3   3 -0.4   4 1.9   5 5.1   6 6.2    Let\u0026rsquo;s start with a basic dot plot of these points. I\u0026rsquo;ll be using the Julia Programming Language for these examples.\nusing StatsPlots # initialize an array of the samples x = [-2.1; -1.3; -0.4; 1.9; 5.1; 6.2]; # plot it out scatter(x, zeros(length(x)), legend = false)  KDE with KernelDensity.jl Now applying the quick and easy solution: a package. This package has a function named kde that takes a one dimensional array (or vector), a bandwidth argument, and a chosen kernel (we\u0026rsquo;ll use the default). So let\u0026rsquo;s see it:\nimport KernelDensity KernelDensity.kde(x, bandwidth = sqrt(2.25)) |\u0026gt; x -\u0026gt; plot!(x, legend = false)  There we go, we\u0026rsquo;ve applied KDE to these data points and we can now see the bimodal nature of this data. If all we wanted to do was visualize the distribution then we\u0026rsquo;re done. I\u0026rsquo;d like to dig a bit deeper though.\nKDE with Distributions.jl What is the kernel part of this about? What was the default kernel we used in the previous section? The kde function from the package used a default kernel associated with the Normal distribution. But to understand what this all means we need to take a look at the definition of Kernel Density Estimation:\n$$ D_h(x; {x_i}) = \\sum_{i=1}^n \\frac{1}{nh} K\\left(\\frac{x - x_i}{h}\\right) $$\nBreaking down this formula a bit: The kernel is the function shown above as $K$ and Janert describes it like so:\n To form a KDE, we place a kernel —that is, a smooth, strongly peaked function—at the position of each data point. We then add up the contributions from all kernels to obtain a smooth curve, which we can evaluate at any point along the x axis\n We are effectively calculating weighted distances from our data points to points along the x axis. There is a great interactive introduction to kernel density estimation here. I highly recommend it because you can play with bandwidth, select different kernel methods, and check out the resulting effects.\nAs I mentioned before, the default kernel for this package is the Normal (or Gaussian) probability density function (pdf):\n$$ K(x) = \\frac{1}{\\sqrt{2\\pi}}\\text{exp}\\left(-\\frac{1}{2}x^2\\right) $$\nSince we are calculating pdfs I\u0026rsquo;ll use the Distributions.jl package to create each distribution, calculate the densities, and sum the results.\nusing Distributions dists = Normal.(x, sqrt(2.25))  6-element Array{Distributions.Normal{Float64},1}: Distributions.Normal{Float64}(μ=-2.1, σ=1.5) Distributions.Normal{Float64}(μ=-1.3, σ=1.5) Distributions.Normal{Float64}(μ=-0.4, σ=1.5) Distributions.Normal{Float64}(μ=1.9, σ=1.5) Distributions.Normal{Float64}(μ=5.1, σ=1.5) Distributions.Normal{Float64}(μ=6.2, σ=1.5)  Here we see a neat feature of the Julia language. Any Julia function can be vectorized (or broadcasted) by the application of the . (or \u0026ldquo;dot\u0026rdquo;) operator. See this blog post if you want to learn more about it. Above we applied the Normal method element-wise creating an array of Normal distributions. The mean of our individual distributions being our data points and a variance of 2.25 (aka our chosen bandwidth). Let\u0026rsquo;s plot each of these distributions:\nplot(dists, legend = false)  Summing up their probability densities across all of x.\n# create an iterator x_d = range(-7, 11, length = 100) # find the kde with a gaussian kernel dens = sum(pdf.(eachdist, x_d) for eachdist in dists) plot!(x_d, dens)  The resulting shape of the KDE is identical to the one we first calculated. We could stop here except this is really just a special case where we are using the gaussian kernel. Let\u0026rsquo;s extrapolate a bit so we could use different kernels.\nKernel Density from Scratch To apply a new kernel method we can just write the KDE code from scratch. Below I\u0026rsquo;ve defined the KDE function as D and the kernel argument as K to mimic the math above.\n# define some kernels: # gaussian kernel kgauss(x) = 1/sqrt(2π) * exp(-1/2 * x^2) # boxcar kbox(x) = abs(x) \u0026lt;= 1 ? 1/2 : 0 # triangular ktri(x) = abs(x) \u0026lt;= 1 ? 1 - abs(x) : 0 # define the KDE function D(x, h, xi, K) = 1/(length(xi) * h) * sum(K.((x .- xi) / h)) # evaluate KDE along the x-axis using comprehensions dens = [D(xstep, sqrt(2.25), x, K) for xstep in x_d, K in (kgauss, kbox, ktri)] # visualize the kernels plot(x_d, dens, label = [\u0026quot;Gaussian\u0026quot;, \u0026quot;Box\u0026quot;, \u0026quot;Triangular\u0026quot;])  In my example above I used some shorthand Julia syntax. The ?: syntax is called a ternary operator and makes a conditional if-else statement more compact. I also used Julia\u0026rsquo;s Assignment form for my function definitions above because it looks a lot more like the math involved. You could have easily defined each of these functions to look more like so:\nfunction foo(x) if ... ... else ... end return ... end  What\u0026rsquo;s next? A short post on cumulative distribution functions (cdf) using Julia will likely follow this one. Janert introduces both kdes and cdfs in his chapter A Single Variable: Shape and Distribution and they complement each other really well. Thanks for reading!\n","date":1552712400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552712400,"objectID":"b74e75123768706f2f14b4ac8374c7e6","permalink":"/post/2019-03-16_kde-scratch/","publishdate":"2019-03-16T00:00:00-05:00","relpermalink":"/post/2019-03-16_kde-scratch/","section":"post","summary":"Motivation Why am I writing about KDE\u0026rsquo;s? At the recommendation of Will Kurt\u0026rsquo;s probability blog I\u0026rsquo;ve been reading a great book on data analysis by Philipp K. Janert:\nThis book has a great intro to KDEs that explain the motivation.","tags":["julia","datasci"],"title":"Kernel Density Estimation from Scratch","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"299a077dcf7d88d57edc9080508f2dd4","permalink":"/bio/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/bio/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"About / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536469200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441083600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441083600,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-05:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372654800,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-05:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"A mobile visual clothing search system is presented whereby a smart phone user can either choose a social networking image or capture a new photo of a person wearing clothing of interest and search for similar clothing in a large cloud-based ecommerce database. The phone's GPS location is used to re-rank results by retail store location, to inform the user of local stores where similar clothing items can be tried on.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":"Welcome to Slides  Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides  Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable","tags":null,"title":"Slides","type":"slides"}]