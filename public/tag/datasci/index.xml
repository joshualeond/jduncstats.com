<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>datasci | Josh Duncan</title>
    <link>/tag/datasci/</link>
      <atom:link href="/tag/datasci/index.xml" rel="self" type="application/rss+xml" />
    <description>datasci</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 16 Mar 2019 00:00:00 -0500</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>datasci</title>
      <link>/tag/datasci/</link>
    </image>
    
    <item>
      <title>Kernel Density Estimation from Scratch</title>
      <link>/post/2019-03-16_kde-scratch/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 -0500</pubDate>
      <guid>/post/2019-03-16_kde-scratch/</guid>
      <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Why am I writing about KDE&amp;rsquo;s? At the recommendation of Will Kurt&amp;rsquo;s 
&lt;a href=&#34;https://www.countbayesie.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;probability blog&lt;/a&gt; I&amp;rsquo;ve been reading a great book on data analysis by Philipp K. Janert:&lt;/p&gt;
&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/janert.jpg&#34; alt=&#34;drawing&#34; width=&#34;50%&#34;/&gt;
&lt;p&gt;This book has a great intro to KDEs that explain the motivation. My own abbreviated version are that KDEs provide a useful technique to visualize a variables distribution. Visualizing your data is an important step to take early in the data analysis stage. In fact, there are many metrics that we commonly use to understand a variable that have an implicit assumption that your data are &lt;code&gt;unimodal&lt;/code&gt; (having a single peak). If your data doesn&amp;rsquo;t have this structure then you may be mislead by measures of central tendency (mean/median/mode), outliers, or other statistical methods (linear regression, t-tests, etc.).&lt;/p&gt;
&lt;p&gt;This post&amp;rsquo;s structure follows closely with how I commonly learn topics. I start at a high level, using a pre-canned solution for the algorithm, and then work backward to find out what&amp;rsquo;s going on underneath.&lt;/p&gt;
&lt;p&gt;I use a small example I discovered on the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel_density_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia page&lt;/a&gt; for KDEs. It uses a handful of data points for a single variable:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-2.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;-1.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;-0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;6.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let&amp;rsquo;s start with a basic &lt;code&gt;dot&lt;/code&gt; plot of these points. I&amp;rsquo;ll be using the &lt;code&gt;Julia Programming Language&lt;/code&gt; for these examples.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using StatsPlots

# initialize an array of the samples
x = [-2.1; -1.3; -0.4; 1.9; 5.1; 6.2];
# plot it out
scatter(x, zeros(length(x)), legend = false)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_1_1.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kde-with-kerneldensityjl&#34;&gt;KDE with KernelDensity.jl&lt;/h2&gt;
&lt;p&gt;Now applying the quick and easy solution: a 
&lt;a href=&#34;https://github.com/JuliaStats/KernelDensity.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package&lt;/a&gt;. This package has a function named &lt;code&gt;kde&lt;/code&gt; that takes a one dimensional array (or vector), a bandwidth argument, and a chosen kernel (we&amp;rsquo;ll use the default). So let&amp;rsquo;s see it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;import KernelDensity

KernelDensity.kde(x, bandwidth = sqrt(2.25)) |&amp;gt;
  x -&amp;gt; plot!(x, legend = false)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_2_1.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;There we go, we&amp;rsquo;ve applied KDE to these data points and we can now see the &lt;code&gt;bimodal&lt;/code&gt; nature of this data. If all we wanted to do was visualize the distribution then we&amp;rsquo;re done. I&amp;rsquo;d like to dig a bit deeper though.&lt;/p&gt;
&lt;h2 id=&#34;kde-with-distributionsjl&#34;&gt;KDE with Distributions.jl&lt;/h2&gt;
&lt;p&gt;What is the &lt;code&gt;kernel&lt;/code&gt; part of this about? What was the default kernel we used in the previous section? The &lt;code&gt;kde&lt;/code&gt; function from the package used a default kernel associated with the Normal distribution. But to understand what this all means we need to take a look at the definition of Kernel Density Estimation:&lt;/p&gt;
&lt;p&gt;$$
D_h(x; {x_i}) = \sum_{i=1}^n \frac{1}{nh} K\left(\frac{x - x_i}{h}\right)
$$&lt;/p&gt;
&lt;p&gt;Breaking down this formula a bit: The kernel is the function shown above as &lt;code&gt;$K$&lt;/code&gt; and Janert describes it like so:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To form a KDE, we place a &lt;em&gt;kernel&lt;/em&gt; —that is, a smooth, strongly peaked function—at the
position of each data point. We then add up the contributions from all kernels to obtain a
smooth curve, which we can evaluate at any point along the &lt;em&gt;x&lt;/em&gt; axis&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are effectively calculating weighted distances from our data points to points along the &lt;em&gt;x&lt;/em&gt; axis. There is a great interactive introduction to kernel density estimation 
&lt;a href=&#34;https://mathisonian.github.io/kde/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. I highly recommend it because you can play with bandwidth, select different kernel methods, and check out the resulting effects.&lt;/p&gt;
&lt;p&gt;As I mentioned before, the default &lt;code&gt;kernel&lt;/code&gt; for this package is the Normal (or Gaussian) probability density function (pdf):&lt;/p&gt;
&lt;p&gt;$$
K(x) = \frac{1}{\sqrt{2\pi}}\text{exp}\left(-\frac{1}{2}x^2\right)
$$&lt;/p&gt;
&lt;p&gt;Since we are calculating pdfs I&amp;rsquo;ll use the 
&lt;a href=&#34;https://github.com/JuliaStats/Distributions.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Distributions.jl&lt;/a&gt; package to create each distribution, calculate the densities, and sum the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions

dists = Normal.(x, sqrt(2.25))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;6-element Array{Distributions.Normal{Float64},1}:
 Distributions.Normal{Float64}(μ=-2.1, σ=1.5)
 Distributions.Normal{Float64}(μ=-1.3, σ=1.5)
 Distributions.Normal{Float64}(μ=-0.4, σ=1.5)
 Distributions.Normal{Float64}(μ=1.9, σ=1.5)
 Distributions.Normal{Float64}(μ=5.1, σ=1.5)
 Distributions.Normal{Float64}(μ=6.2, σ=1.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see a neat feature of the Julia language. Any Julia function can be vectorized (or broadcasted) by the application of the &lt;code&gt;.&lt;/code&gt; (or &amp;ldquo;dot&amp;rdquo;) operator. See 
&lt;a href=&#34;https://julialang.org/blog/2018/05/extensible-broadcast-fusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; blog post if you want to learn more about it. Above we applied the &lt;code&gt;Normal&lt;/code&gt; method element-wise creating an array of Normal distributions. The mean of our individual distributions being our data points and a variance of &lt;code&gt;2.25&lt;/code&gt; (aka our chosen bandwidth). Let&amp;rsquo;s plot each of these distributions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;plot(dists, legend = false)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_4_1.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Summing up their probability densities across all of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# create an iterator
x_d = range(-7, 11, length = 100)
# find the kde with a gaussian kernel
dens = sum(pdf.(eachdist, x_d) for eachdist in dists)

plot!(x_d, dens)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_5_1.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The resulting shape of the KDE is identical to the one we first calculated. We could stop here except this is really just a special case where we are using the gaussian kernel. Let&amp;rsquo;s extrapolate a bit so we could use different kernels.&lt;/p&gt;
&lt;h2 id=&#34;kernel-density-from-scratch&#34;&gt;Kernel Density from Scratch&lt;/h2&gt;
&lt;p&gt;To apply a new kernel method we can just write the KDE code from scratch. Below I&amp;rsquo;ve defined the KDE function as &lt;code&gt;D&lt;/code&gt; and the kernel argument as &lt;code&gt;K&lt;/code&gt; to mimic the math above.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# define some kernels:
# gaussian kernel
kgauss(x) = 1/sqrt(2π) * exp(-1/2 * x^2)
# boxcar
kbox(x) = abs(x) &amp;lt;= 1 ? 1/2 : 0
# triangular
ktri(x) = abs(x) &amp;lt;= 1 ? 1 - abs(x) : 0

# define the KDE function
D(x, h, xi, K) =
  1/(length(xi) * h) * sum(K.((x .- xi) / h))

# evaluate KDE along the x-axis using comprehensions
dens = [D(xstep, sqrt(2.25), x, K) for xstep in x_d, K in (kgauss, kbox, ktri)]

# visualize the kernels
plot(x_d, dens, label = [&amp;quot;Gaussian&amp;quot;, &amp;quot;Box&amp;quot;, &amp;quot;Triangular&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-16-kde-scratch_files/2019-03-16_kde-scratch_6_1.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my example above I used some shorthand Julia syntax. The &lt;code&gt;?:&lt;/code&gt; syntax is called a ternary operator and makes a conditional &lt;code&gt;if-else&lt;/code&gt; statement more compact. I also used Julia&amp;rsquo;s &lt;code&gt;Assignment&lt;/code&gt; form for my function definitions above because it looks a lot more like the math involved. You could have easily defined each of these functions to look more like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function foo(x)
  if ...
    ...
  else
    ...
  end
  return ...
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;
&lt;p&gt;A short post on cumulative distribution functions (cdf) using Julia will likely follow this one. Janert introduces both kdes and cdfs in his chapter &lt;strong&gt;A Single Variable: Shape and Distribution&lt;/strong&gt; and they complement each other really well. Thanks for reading!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
