---
title: "Chapter 1"
description: |
  Reproducing Chapter 1 of Bayesian Methods for Hackers in R + Stan
author:
  - name: J. Duncan
date: 01-08-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1.4 Using Computers to Perform Bayesian Inference for Us

### 1.4.1 Example: Inferring Behavior from Text-Message Data

```{r}
library(tidyverse)
library(cmdstanr)
library(posterior)
library(bayesplot)
library(tidybayes)
library(latex2exp)

url <- "https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/data/txtdata.csv"

data <- read_csv(url, col_names = FALSE) %>% 
  rename(count = X1) %>% 
  mutate(day = row_number())
```

Let's visualize this data to make sure all is well:

```{r}
theme_set(theme_tidybayes())

data %>% 
  ggplot(aes(x=day, y=count)) + 
  geom_col() +
  labs(
    title = "Did the user's texting habits change over time?",
    x = "Time (days)",
    y = "count of text-msgs received"
  ) 
```

### 1.4.2 Introducing Our First Hammer: ~~PyMC~~ Stan

This is where things get interesting. The original model included a discrete random variable $\tau$ that was modeled with a *Discrete Uniform* distribution. Then they sampled with the *Metropolis* algorithm so it was possible to sample from a joint distribution including this discrete parameter.

A limitation (I suppose) of the main algorithm used by Stan, HMC, is that everything must be differentiable and so discrete parameters are off the table. However, with a little statistical theory you're still able to estimate this model and even the posterior of $\tau$ by marginalizing the parameter out of the probability function. Then recovering this parameter afterwards in the **generated quantities** block!

See the [Stan](https://mc-stan.org/docs/2_25/stan-users-guide/change-point-section.html) manual as well as [this](https://nowave.it/pages/bayesian-changepoint-detection-with-r-and-stan.html) blog post for more info on implementing this model and the marginalization process.

```{r}
data_list <- tidybayes::compose_data(data)
data_list[['alpha']] <- 1 / mean(data_list$count)
data_list
```

Here's the Stan code used for estimating this joint probability distribution:

```{stan, output.var = "modprint", eval = FALSE}
data {
  int<lower=1> n;
  int<lower=0> count[n];
  real alpha;
}

transformed data {
  real log_unif;
  log_unif = -log(n);
}

parameters {
  real<lower=0> lambda_1;
  real<lower=0> lambda_2;
}

transformed parameters {
  vector[n] lp;
  lp = rep_vector(log_unif, n);
  for (tau in 1:n)
    for (i in 1:n)
      lp[tau] += poisson_lpmf(count[i] | i < tau ? lambda_1 : lambda_2);
}

model {
  lambda_1 ~ exponential(alpha);
  lambda_2 ~ exponential(alpha);
  target += log_sum_exp(lp);
}

generated quantities {
  int<lower=1,upper=n> tau;
  vector<lower=0>[n] expected;
  // posterior of discrete change point
  tau = categorical_logit_rng(lp);
  // predictions for each day
  for (day in 1:n)
    expected[day] = day < tau ? lambda_1 : lambda_2;
}

```

Now for sampling from the distribution and obtaining the marginal distributions:

```{r}
mod <- cmdstanr::cmdstan_model("models/ch1-mod.stan")

fit <- mod$sample(
  data = data_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  refresh = 1000,
  iter_sampling = 3000
)

fit$summary()
```

```{r}
# https://mpopov.com/blog/2020/09/07/pivoting-posteriors/
tidy_draws.CmdStanMCMC <- function(model, ...) {
  return(as_draws_df(model$draws()))
}
```

Let's visualize the marginal distributions of $\lambda_1$ and $\lambda_2$:

```{r}
draws_df <- as_draws_df(fit$draws(c("lambda_1", "lambda_2", "tau")))

draws_df %>% 
  gather_draws(lambda_1, lambda_2) %>%
  ggplot(aes(y = .variable, x = .value)) +
  stat_dotsinterval(quantiles = 100) +
  labs(
    title = TeX("Posterior Distributions of $\\lambda_1$ and $\\lambda_2$"),
    y = NULL
  )
```

Reviewing when the change point occurred:

```{r}
draws_df %>% 
  gather_draws(tau) %>% 
  ggplot(aes(x = .value)) +
  geom_bar(aes(y = ..count../sum(..count..))) +
  scale_x_continuous(breaks = scales::pretty_breaks(10), limits = c(40, 50)) +
  labs(
    title = TeX("Posterior Distribution of $\\tau$"),
    y = "probability",
    x = TeX("$\\tau$ (in days)")
  ) 
```

> Our analysis also returned a distribution for $\tau$. Its posterior distribution looks a little different from the other two because it is a discrete random variable, so it doesn't assign probabilities to intervals. We can see that near day 46, there was a 50% chance that the user's behaviour changed. Had no change occurred, or had the change been gradual over time, the posterior distribution of $\tau$ would have been more spread out, reflecting that many days were plausible candidates for $\tau$. By contrast, in the actual results we see that only three or four days make any sense as potential transition points.

### 1.4.4 What Good Are Samples from the Posterior, Anyways?

Now for calculating the expected values:

```{r}
predictions <- fit$draws("expected") %>% 
  as_draws_df() %>% 
  spread_draws(expected[day])

predictions
```

Let's visualize these predictions now including the uncertainty around our $\lambda$ parameters:

```{r}
predictions %>% 
  mean_qi(.width = c(.99, .95, .8)) %>% 
  ggplot(aes(x = day)) +
  geom_col(data = data, aes(y = count)) +
  geom_lineribbon(aes(y = expected, ymin = .lower, ymax = .upper), color = "#08519C") +
  labs(
    title = "Expected number of text-messages received",
    x = "Time (days)",
    y = "Expected # text-messages"
  ) +
  scale_fill_brewer() 
```

## 1.6 Appendix
### 1.6.1 Determining Statistically if the Two $\lambda$s Are Indeed Different?

Now for reviewing the difference between these two $\lambda$'s. In the book he answers this question with explicit probabilities but we can also represent the same question with a data visualization. This shows us that the probability that these $\lambda$s differ by at least 5 times is `50`%. 

```{r}
draws_df %>% 
  gather_draws(lambda_1, lambda_2) %>%
  compare_levels(.value, by = .variable) %>% 
  ggplot(aes(y = .variable, x = .value, fill = stat(abs(x) < 5))) +
  stat_halfeye() +
  geom_vline(xintercept = c(-5,5), linetype = "dashed") +
  scale_fill_manual(values = c("gray80", "skyblue")) +
  labs(
    title = "Comparing our parameters",
    y = NULL
  )
```

