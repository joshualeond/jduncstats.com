---
title: "Homework 2, Chapter 4"
author: "J. Duncan"
date: 2019-03-10T21:13:14-05:00
draft: true
tags:
- rstats
- bayes
output:
  blogdown::html_page:
    dev: "svg"
---



<div id="hard-problems" class="section level2">
<h2>Hard Problems</h2>
<div id="h1." class="section level3">
<h3>4H1.</h3>
<p>The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals (either HPDI or PI) for each of these individuals. That is, fill in the table below, using model-based predictions.</p>
<pre class="r"><code>library(tidyverse)
library(knitr)
library(kableExtra)

kung &lt;- tibble::tribble(
  ~individual, ~weight,
            1,   46.95,
            2,   43.72,
            3,   64.78,
            4,   32.59,
            5,   54.63
  )

# kable(kung, digits = 3, row.names = FALSE, align = &quot;c&quot;, caption = NULL) %&gt;%
#   kable_styling(bootstrap_options = &quot;striped&quot;, full_width = FALSE)</code></pre>
<p>Well it looks like first we need to estimate this model. In the book on page <code>96</code> we see the MAP estimation of this model on the filtered dataset. I’m going to fit this one for predictions.</p>
<pre class="r"><code>library(rethinking)
data(&quot;Howell1&quot;)

d &lt;- Howell1
d2 &lt;- d[d$age &gt;= 18, ]

# fit model
m4.3 &lt;- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma), # likelihood
    mu &lt;- a + b * weight, # linear model
    a ~ dnorm(156, 100), # prior
    b ~ dnorm(0, 10), # prior
    sigma ~ dunif(0, 50) # prior
  ),
  data = d2
)</code></pre>
<p>Now the model is fit and we can estimate the predictions in two ways:</p>
<ol style="list-style-type: decimal">
<li>Collect posterior samples of the parameters of the estimated model and for each weight in the above dataframe sample from a Normal distribution with our posterior samples as parameters</li>
<li>Use the convenience function in the <code>rethinking</code> package called <code>sim()</code> to do the above giving it only the new weights</li>
</ol>
<pre class="r"><code>post &lt;- extract.samples(m4.3)
head(post)</code></pre>
<pre><code>##          a         b    sigma
## 1 112.0734 0.9424389 5.235063
## 2 112.0144 0.9380622 5.386037
## 3 112.2121 0.9355411 5.176812
## 4 113.2801 0.9155520 5.131002
## 5 112.9359 0.9258103 4.965042
## 6 114.4667 0.8890500 5.054368</code></pre>
<pre class="r"><code>sim.height &lt;- sapply(kung$weight, function(weight)
  rnorm(
    n = nrow(post),
    mean = post$a + post$b * weight,
    sd = post$sigma
    )
  )

(sim.PI &lt;- apply(sim.height, 2, PI, prob = 0.89))</code></pre>
<pre><code>##         [,1]     [,2]     [,3]     [,4]     [,5]
## 5%  148.1218 145.3271 164.2434 135.3212 155.2902
## 94% 164.5610 161.6145 180.6696 151.4692 171.4851</code></pre>
<p>And the convenient way:</p>
<pre class="r"><code>sim.height &lt;- sim(m4.3, data = list(weight = kung$weight))</code></pre>
<pre><code>## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]</code></pre>
<pre class="r"><code>(sim.PI &lt;- apply(sim.height, 2, PI, prob = 0.89))</code></pre>
<pre><code>##         [,1]     [,2]     [,3]     [,4]     [,5]
## 5%  148.5534 144.9224 163.5080 134.9054 155.4800
## 94% 164.3544 161.6533 180.2119 151.1621 171.0385</code></pre>
<p>We have relatively the same results here which is expected. Let’s get the mean values as well:</p>
<pre class="r"><code>sim.mean &lt;- apply(sim.height, 2, mean)</code></pre>
<p>Now let’s put all of this in the dataframe:</p>
<pre class="r"><code>kung$exp_height &lt;- sim.mean
kung$lo_89 &lt;- sim.PI[1,]
kung$hi_89 &lt;- sim.PI[2,]

# kable(kung, digits = 3, row.names = FALSE, align = &quot;c&quot;, caption = NULL) %&gt;%
#   kable_styling(bootstrap_options = &quot;striped&quot;, full_width = FALSE)</code></pre>
</div>
</div>
<div id="h2." class="section level2">
<h2>4H2.</h2>
<p>Select out all the rows in the <code>Howell1</code> data with ages below 18 years of age. If you do it right you should end up with a new data frame with 192 rows in it.</p>
<pre class="r"><code>d3 &lt;- d[d$age &lt; 18,]</code></pre>
<div id="a" class="section level3">
<h3>(a)</h3>
<blockquote>
<p>Fit a linear regression to these data, using <code>map</code>. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?</p>
</blockquote>
<pre class="r"><code>fit &lt;- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma), # likelihood
    mu &lt;- a + b * weight, # linear model
    a ~ dnorm(156, 100), # prior
    b ~ dnorm(0, 10), # prior
    sigma ~ dunif(0, 50) # prior
  ),
  data = d3
)</code></pre>
<p>Let’s look at the results of this model:</p>
<pre class="r"><code>post_early &lt;- extract.samples(fit)
precis(post_early)</code></pre>
<pre><code>##        Mean StdDev |0.89 0.89|
## a     58.26   1.40 56.11 60.58
## b      2.72   0.07  2.61  2.83
## sigma  8.44   0.43  7.72  9.08</code></pre>
<p>So we see for every <code>1</code> unit increase in weight a child should be <code>2.72</code> cm taller. Or as the question has asked <code>10</code> unit increase is a <code>27.2</code> cm increase in height. For every <code>22</code> lb increase there is a corresponding ~<code>1</code> foot increase in height.</p>
</div>
<div id="b" class="section level3">
<h3>(b)</h3>
<blockquote>
<p>Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% HPDI for the mean. Also superimpose the 89% HPDI for predicted heights.</p>
</blockquote>
<pre class="r"><code>plot(height ~ weight, d3, col = col.alpha(rangi2, 0.5))

# get index for calculations
weight.seq &lt;- seq(0, 50, by = 1)

# extract samples of parameters and calculate expected mu
mu &lt;- link(fit, data = data.frame(weight = weight.seq))</code></pre>
<pre><code>## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]</code></pre>
<pre class="r"><code>mu.mean &lt;- apply(mu, 2, mean)
lines(weight.seq, mu.mean)
mu.HPDI &lt;- apply(mu, 2, HPDI)
shade(mu.HPDI, weight.seq)

# simulate posterior observations of the model fit
sim.height &lt;- sim(fit, data = data.frame(weight = weight.seq))</code></pre>
<pre><code>## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]</code></pre>
<pre class="r"><code>height.HPDI &lt;- apply(sim.height, 2, HPDI)
shade(height.HPDI, weight.seq)</code></pre>
<p><img src="/post/hw2-ch-4_files/figure-html/unnamed-chunk-10-1.svg" width="672" /></p>
</div>
<div id="c" class="section level3">
<h3>(c)</h3>
<blockquote>
<p>What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model.</p>
</blockquote>
<p>Well the model appears to be doing good job with the data at hand. The prior’s were set for persons age &gt; 18 so these should be adjusted. Something of conrern is that you see a non-linear structure in this data. So the two tails of the data are not covered well by the model. This non-linearity could be accounted for with a polynomial regression or with splines.</p>
</div>
</div>
<div id="h3." class="section level2">
<h2>4H3.</h2>
<p>Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the <strong>logarithm</strong> of body weight that scales with height!” Let’s take your colleague’s advice and see what happens.</p>
<div id="a-1" class="section level3">
<h3>(a)</h3>
<blockquote>
<p>Model the relationship between height(cm) and the natural logarithm of weight (log-kg). Use the entire <code>Howell1</code> data frame, all 544 rows, adults and non-adults. Fit this model, using quadratic approximation:</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
h_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta \text{log}(w_i) \\
\alpha \sim \text{Normal}(178, 100) \\
\beta \sim \text{Normal}(0, 100) \\
\sigma \sim \text{Uniform}(0, 50)
\end{aligned}
\]</span>
where <span class="math inline">\(h_i\)</span> is the height of individual <span class="math inline">\(i\)</span> and <span class="math inline">\(w_i\)</span> is the weight (in kg) of individual <span class="math inline">\(i\)</span>. The function for computing a natural log in R is just <code>log</code>. Can you interpret the resulting estimates?</p>
<pre class="r"><code>fit_all &lt;- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu &lt;- a + b * log(weight), # linear model
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 100),
    sigma ~ dunif(0, 50)
  ),
  data = d
)</code></pre>
<p>Now the model is fit let’s review the coefficients.</p>
<pre class="r"><code>precis(fit_all)</code></pre>
<pre><code>##         Mean StdDev   5.5%  94.5%
## a     -23.78   1.34 -25.92 -21.65
## b      47.07   0.38  46.46  47.69
## sigma   5.13   0.16   4.89   5.38</code></pre>
<pre class="r"><code>plot(precis(fit_all))</code></pre>
<p><img src="/post/hw2-ch-4_files/figure-html/unnamed-chunk-12-1.svg" width="672" /></p>
<p>When the weight is low or equivalent to <code>1</code> then the height would be <code>-23</code> which is not insightful. Given that this is a log-level model we can <a href="https://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor">interpret</a> our <span class="math inline">\(\beta\)</span> as a one percent increase in weight corresponds to a <span class="math inline">\(\beta/100 = 47.08/100\)</span> unit increase in height.</p>
</div>
<div id="b-1" class="section level3">
<h3>(b)</h3>
<p>Begin with this plot:</p>
<pre class="r"><code>plot(height ~ weight, data = d, col = col.alpha(rangi2, 0.4))</code></pre>
<p><img src="/post/hw2-ch-4_files/figure-html/unnamed-chunk-13-1.svg" width="672" /></p>
<p>Then use the samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% HPDI for the mean, and (3) the 97% HPDI for predicted heights.</p>
<pre class="r"><code>weight.seq &lt;- seq(2, 65, by = 1)
all_samp &lt;- link(fit_all, data = data.frame(weight = weight.seq))</code></pre>
<pre><code>## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]</code></pre>
<pre class="r"><code>all_mu &lt;- apply(all_samp, 2, mean)
all_mu.HPDI &lt;- apply(all_samp, 2, HPDI, prob = 0.97)

all_sim &lt;- sim(fit_all, data = data.frame(weight = weight.seq))</code></pre>
<pre><code>## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]</code></pre>
<pre class="r"><code>all_sim.HPDI &lt;- apply(all_sim, 2, HPDI, prob = 0.97)

plot(height ~ weight, data = d, col = col.alpha(rangi2, 0.4))
# the predicted mean height
lines(weight.seq, all_mu)
# the 97% HPDI of the mean
shade(all_mu.HPDI, weight.seq)
# the 97% HPDI for predictions
shade(all_sim.HPDI, weight.seq)</code></pre>
<p><img src="/post/hw2-ch-4_files/figure-html/unnamed-chunk-14-1.svg" width="672" /></p>
</div>
</div>
